Image classification with Swin Transformers

Swin Transformer (Shifted Window Transformer) can serve as a general-purpose backbone for computer vision. Swin Transformer is a hierarchical Transformer whose representations are computed with shifted windows. The shifted window scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connections. This architecture has the flexibility to model information at various scales and has a linear computational complexity with respect to image size.

This example requires TensorFlow 2.5 or higher, as well as TensorFlow Addons, which can be installed using the following commands:

!pip install -U tensorflow-addons
Requirement already satisfied: tensorflow-addons in /opt/conda/lib/python3.7/site-packages (0.14.0)
Collecting tensorflow-addons
  Downloading tensorflow_addons-0.19.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 21.6 MB/s eta 0:00:0000:01
Requirement already satisfied: typeguard>=2.7 in /opt/conda/lib/python3.7/site-packages (from tensorflow-addons) (2.13.3)
Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from tensorflow-addons) (21.3)
Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->tensorflow-addons) (3.0.7)
Installing collected packages: tensorflow-addons
  Attempting uninstall: tensorflow-addons
    Found existing installation: tensorflow-addons 0.14.0
    Uninstalling tensorflow-addons-0.14.0:
      Successfully uninstalled tensorflow-addons-0.14.0
Successfully installed tensorflow-addons-0.19.0
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv
Setup
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
import tensorflow_addons as tfa
from tensorflow import keras
from tensorflow.keras import layers
import os
/opt/conda/lib/python3.7/site-packages/tensorflow_addons/utils/ensure_tf_install.py:67: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.9.0 and strictly below 2.12.0 (nightly versions are not supported). 
 The versions of TensorFlow you are currently using is 2.6.3 and is not supported. 
Some things might work, some things might not.
If you were to encounter a bug, do not file an issue.
If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. 
You can find the compatibility matrix in TensorFlow Addon's readme:
https://github.com/tensorflow/addons
  UserWarning,
import numpy as np
import os
from sklearn.metrics import confusion_matrix
#import seaborn as sn; sn.set(font_scale=1.4)
from sklearn.utils import shuffle           
import matplotlib.pyplot as plt             
import cv2                                 
import tensorflow as tf                
from tqdm import tqdm
#from sklearn.metrics import classification_report, log_loss, accuracy_score
from keras.models import Sequential
from keras.layers import Dense, Dropout, Conv2D, MaxPool2D, Flatten
from keras.utils import np_utils
import tensorflow as tf
import datetime
import numpy as np
import tensorflow as tf
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
class_names = ['glioma_tumor','meningioma_tumor','no_tumor','pituitary_tumor']
class_names_label = {class_name:i for i, class_name in enumerate(class_names)}

nb_classes = len(class_names)

IMAGE_SIZE = (128, 128)
def load_data():
    """
        Load the data:
            - 14,034 images to train the network.
            - 3,000 images to evaluate how accurately the network learned to classify images.
    """
    TrainF=r"/kaggle/input/brain-tumor-classification-mri/Training"
    TestF=r"/kaggle/input/brain-tumor-classification-mri/Testing"
    datasets =  [TrainF,TestF]
    output = []
    
    # Iterate through training and test sets
    for dataset in datasets:
        
        images = []
        labels = []
        
        print("Loading {}".format(dataset))
        
        # Iterate through each folder corresponding to a category
        for folder in os.listdir(dataset):
            label = class_names_label[folder]
            
            # Iterate through each image in our folder
            for file in tqdm(os.listdir(os.path.join(dataset, folder))):
                
                # Get the path name of the image
                img_path = os.path.join(os.path.join(dataset, folder), file)
                
                # Open and resize the img
                image = cv2.imread(img_path)
                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
                image = cv2.resize(image, IMAGE_SIZE) 
                
                # Append the image and its corresponding label to the output
                images.append(image)
                labels.append(label)
                
        images = np.array(images, dtype = 'float32')
        labels = np.array(labels, dtype = 'int32')   
        
        output.append((images, labels))

    return output
num_classes=4
(x_train, y_train), (x_test, y_test) = load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)
print(f"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}")
print(f"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}")

#plt.figure(figsize=(10, 10))
#for i in range(25):
#    plt.subplot(5, 5, i + 1)
#    plt.xticks([])
#    plt.yticks([])
#    plt.grid(False)
#    plt.imshow(x_train[i])
#plt.show()
Loading /kaggle/input/brain-tumor-classification-mri/Training
100%|██████████| 395/395 [00:03<00:00, 117.00it/s]
100%|██████████| 827/827 [00:08<00:00, 96.72it/s] 
100%|██████████| 822/822 [00:07<00:00, 115.96it/s]
100%|██████████| 826/826 [00:07<00:00, 113.40it/s]
Loading /kaggle/input/brain-tumor-classification-mri/Testing
100%|██████████| 105/105 [00:00<00:00, 163.68it/s]
100%|██████████| 74/74 [00:00<00:00, 78.32it/s]
100%|██████████| 115/115 [00:00<00:00, 128.53it/s]
100%|██████████| 100/100 [00:00<00:00, 110.89it/s]
x_train shape: (2870, 128, 128, 3) - y_train shape: (2870, 4)
x_test shape: (394, 128, 128, 3) - y_test shape: (394, 4)
Configure the hyperparameters
A key parameter to pick is the patch_size, the size of the input patches. In order to use each pixel as an individual input, you can set patch_size to (1, 1). Below, we take inspiration from the original paper settings for training on ImageNet-1K, keeping most of the original settings for this example.

from keras.preprocessing.image import ImageDataGenerator
import pandas as pd
tumor_dir=r'/kaggle/input/brain-tumor-mri-dataset/Training/glioma'
tumor_dir1=r'/kaggle/input/brain-tumor-mri-dataset/Training/meningioma'
tumor_dir2=r'/kaggle/input/brain-tumor-mri-dataset/Training/pituitary'
healthy_dir=r'/kaggle/input/brain-tumor-mri-dataset/Training/notumor'
filepaths = []
labels= []
dict_list = [tumor_dir,tumor_dir1, tumor_dir2, healthy_dir]
for i, j in enumerate(dict_list):
    flist=os.listdir(j)
    for f in flist:
        fpath=os.path.join(j,f)
        filepaths.append(fpath)
        if i==0:
          labels.append('glioma_tumor')
        elif i==1:
          labels.append('meningioma_tumor')
        elif i==2:
          labels.append('pituitary_tumor')
        else:
          labels.append('no_tumor')
    
Fseries = pd.Series(filepaths, name="filepaths")
Lseries = pd.Series(labels, name="labels")
tumor_data = pd.concat([Fseries,Lseries], axis=1)
tumor_df = pd.DataFrame(tumor_data)
print(tumor_df.head())
print(tumor_df["labels"].value_counts())

#shape of datatset
tumor_df.shape
                                           filepaths        labels
0  /kaggle/input/brain-tumor-mri-dataset/Training...  glioma_tumor
1  /kaggle/input/brain-tumor-mri-dataset/Training...  glioma_tumor
2  /kaggle/input/brain-tumor-mri-dataset/Training...  glioma_tumor
3  /kaggle/input/brain-tumor-mri-dataset/Training...  glioma_tumor
4  /kaggle/input/brain-tumor-mri-dataset/Training...  glioma_tumor
no_tumor            1595
pituitary_tumor     1457
meningioma_tumor    1339
glioma_tumor        1321
Name: labels, dtype: int64
(5712, 2)
from sklearn.model_selection import train_test_split 
train_images, test_images = train_test_split(tumor_df, test_size=0.15, random_state=42)
train_set, val_set = train_test_split(tumor_df, test_size=0.2, random_state=42)
input_shape=(128,128,3)
patch_size = (32, 32)  # 2-by-2 sized patches
dropout_rate = 0.03  # Dropout rate
num_heads =8  # Attention heads
embed_dim = 64  # Embedding dimension
num_mlp = 256  # MLP layer size
qkv_bias = True  # Convert embedded patches to query, key, and values with a learnable additive value
window_size = 2  # Size of attention window
shift_size = 1  # Size of shifting window
image_dimension = 128  # Initial image size

num_patch_x = input_shape[0] // patch_size[0]
num_patch_y = input_shape[1] // patch_size[1]

learning_rate = 1e-3
batch_size = 64
num_epochs = 40
validation_split = 0.1
weight_decay = 0.0001
label_smoothing = 0.1
patch_size = (10, 10)  # Larger patch size to capture more context
dropout_rate = 0.2  # Slightly higher dropout rate for larger input
num_heads = 8  # Keep the same number of attention heads
embed_dim = 128  # Increase embedding dimension for larger input
num_mlp = 512  # Increase MLP layer size for more complex data
qkv_bias = True  # Keep the same
window_size = 5  # Increase attention window size for larger context
shift_size = 2  # Increase shifting window for more overlap
image_dimension = 150  # Update initial image size to match the new input

num_patch_x = input_shape[0] // patch_size[0]
num_patch_y = input_shape[1] // patch_size[1]

learning_rate = 1e-3
batch_size = 32  # Reduce batch size for larger input (adjust as needed)
num_epochs = 40
validation_split = 0.1
weight_decay = 0.0001
label_smoothing = 0.1
Helper functions
We create two helper functions to help us get a sequence of patches from the image, merge patches, and apply dropout.

def window_partition(x, window_size):
    _, height, width, channels = x.shape
    patch_num_y = height // window_size
    patch_num_x = width // window_size
    x = tf.reshape(
        x, shape=(-1, patch_num_y, window_size, patch_num_x, window_size, channels)
    )
    x = tf.transpose(x, (0, 1, 3, 2, 4, 5))
    windows = tf.reshape(x, shape=(-1, window_size, window_size, channels))
    return windows


def window_reverse(windows, window_size, height, width, channels):
    patch_num_y = height // window_size
    patch_num_x = width // window_size
    x = tf.reshape(
        windows,
        shape=(-1, patch_num_y, patch_num_x, window_size, window_size, channels),
    )
    x = tf.transpose(x, perm=(0, 1, 3, 2, 4, 5))
    x = tf.reshape(x, shape=(-1, height, width, channels))
    return x


class DropPath(layers.Layer):
    def __init__(self, drop_prob=None, **kwargs):
        super(DropPath, self).__init__(**kwargs)
        self.drop_prob = drop_prob

    def call(self, x):
        input_shape = tf.shape(x)
        batch_size = input_shape[0]
        rank = x.shape.rank
        shape = (batch_size,) + (1,) * (rank - 1)
        random_tensor = (1 - self.drop_prob) + tf.random.uniform(shape, dtype=x.dtype)
        path_mask = tf.floor(random_tensor)
        output = tf.math.divide(x, 1 - self.drop_prob) * path_mask
        return output
Window based multi-head self-attention
Usually Transformers perform global self-attention, where the relationships between a token and all other tokens are computed. The global computation leads to quadratic complexity with respect to the number of tokens. Here, as the original paper suggests, we compute self-attention within local windows, in a non-overlapping manner. Global self-attention leads to quadratic computational complexity in the number of patches, whereas window-based self-attention leads to linear complexity and is easily scalable.

class WindowAttention(layers.Layer):
    def __init__(
        self, dim, window_size, num_heads, qkv_bias=True, dropout_rate=0.0, **kwargs
    ):
        super(WindowAttention, self).__init__(**kwargs)
        self.dim = dim
        self.window_size = window_size
        self.num_heads = num_heads
        self.scale = (dim // num_heads) ** -0.5
        self.qkv = layers.Dense(dim * 3, use_bias=qkv_bias)
        self.dropout = layers.Dropout(dropout_rate)
        self.proj = layers.Dense(dim)

    def build(self, input_shape):
        num_window_elements = (2 * self.window_size[0] - 1) * (
            2 * self.window_size[1] - 1
        )
        self.relative_position_bias_table = self.add_weight(
            shape=(num_window_elements, self.num_heads),
            initializer=tf.initializers.Zeros(),
            trainable=True,
        )
        coords_h = np.arange(self.window_size[0])
        coords_w = np.arange(self.window_size[1])
        coords_matrix = np.meshgrid(coords_h, coords_w, indexing="ij")
        coords = np.stack(coords_matrix)
        coords_flatten = coords.reshape(2, -1)
        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]
        relative_coords = relative_coords.transpose([1, 2, 0])
        relative_coords[:, :, 0] += self.window_size[0] - 1
        relative_coords[:, :, 1] += self.window_size[1] - 1
        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1
        relative_position_index = relative_coords.sum(-1)

        self.relative_position_index = tf.Variable(
            initial_value=tf.convert_to_tensor(relative_position_index), trainable=False
        )

    def call(self, x, mask=None):
        _, size, channels = x.shape
        head_dim = channels // self.num_heads
        x_qkv = self.qkv(x)
        x_qkv = tf.reshape(x_qkv, shape=(-1, size, 3, self.num_heads, head_dim))
        x_qkv = tf.transpose(x_qkv, perm=(2, 0, 3, 1, 4))
        q, k, v = x_qkv[0], x_qkv[1], x_qkv[2]
        q = q * self.scale
        k = tf.transpose(k, perm=(0, 1, 3, 2))
        attn = q @ k

        num_window_elements = self.window_size[0] * self.window_size[1]
        relative_position_index_flat = tf.reshape(
            self.relative_position_index, shape=(-1,)
        )
        relative_position_bias = tf.gather(
            self.relative_position_bias_table, relative_position_index_flat
        )
        relative_position_bias = tf.reshape(
            relative_position_bias, shape=(num_window_elements, num_window_elements, -1)
        )
        relative_position_bias = tf.transpose(relative_position_bias, perm=(2, 0, 1))
        attn = attn + tf.expand_dims(relative_position_bias, axis=0)

        if mask is not None:
            nW = mask.get_shape()[0]
            mask_float = tf.cast(
                tf.expand_dims(tf.expand_dims(mask, axis=1), axis=0), tf.float32
            )
            attn = (
                tf.reshape(attn, shape=(-1, nW, self.num_heads, size, size))
                + mask_float
            )
            attn = tf.reshape(attn, shape=(-1, self.num_heads, size, size))
            attn = keras.activations.softmax(attn, axis=-1)
        else:
            attn = keras.activations.softmax(attn, axis=-1)
        attn = self.dropout(attn)

        x_qkv = attn @ v
        x_qkv = tf.transpose(x_qkv, perm=(0, 2, 1, 3))
        x_qkv = tf.reshape(x_qkv, shape=(-1, size, channels))
        x_qkv = self.proj(x_qkv)
        x_qkv = self.dropout(x_qkv)
        return x_qkv
The complete Swin Transformer model
Finally, we put together the complete Swin Transformer by replacing the standard multi-head attention (MHA) with shifted windows attention. As suggested in the original paper, we create a model comprising of a shifted window-based MHA layer, followed by a 2-layer MLP with GELU nonlinearity in between, applying LayerNormalization before each MSA layer and each MLP, and a residual connection after each of these layers.

Notice that we only create a simple MLP with 2 Dense and 2 Dropout layers. Often you will see models using ResNet-50 as the MLP which is quite standard in the literature. However in this paper the authors use a 2-layer MLP with GELU nonlinearity in between.

class SwinTransformer(layers.Layer):
    def __init__(
        self,
        dim,
        num_patch,
        num_heads,
        window_size=7,
        shift_size=0,
        num_mlp=1024,
        qkv_bias=True,
        dropout_rate=0.0,
        **kwargs,
    ):
        super(SwinTransformer, self).__init__(**kwargs)

        self.dim = dim  # number of input dimensions
        self.num_patch = num_patch  # number of embedded patches
        self.num_heads = num_heads  # number of attention heads
        self.window_size = window_size  # size of window
        self.shift_size = shift_size  # size of window shift
        self.num_mlp = num_mlp  # number of MLP nodes

        self.norm1 = layers.LayerNormalization(epsilon=1e-5)
        self.attn = WindowAttention(
            dim,
            window_size=(self.window_size, self.window_size),
            num_heads=num_heads,
            qkv_bias=qkv_bias,
            dropout_rate=dropout_rate,
        )
        self.drop_path = DropPath(dropout_rate)
        self.norm2 = layers.LayerNormalization(epsilon=1e-5)

        self.mlp = keras.Sequential(
            [
                layers.Dense(num_mlp),
                layers.Activation(keras.activations.gelu),
                layers.Dropout(dropout_rate),
                layers.Dense(dim),
                layers.Dropout(dropout_rate),
            ]
        )

        if min(self.num_patch) < self.window_size:
            self.shift_size = 0
            self.window_size = min(self.num_patch)

    def build(self, input_shape):
        if self.shift_size == 0:
            self.attn_mask = None
        else:
            height, width = self.num_patch
            h_slices = (
                slice(0, -self.window_size),
                slice(-self.window_size, -self.shift_size),
                slice(-self.shift_size, None),
            )
            w_slices = (
                slice(0, -self.window_size),
                slice(-self.window_size, -self.shift_size),
                slice(-self.shift_size, None),
            )
            mask_array = np.zeros((1, height, width, 1))
            count = 0
            for h in h_slices:
                for w in w_slices:
                    mask_array[:, h, w, :] = count
                    count += 1
            mask_array = tf.convert_to_tensor(mask_array)

            # mask array to windows
            mask_windows = window_partition(mask_array, self.window_size)
            mask_windows = tf.reshape(
                mask_windows, shape=[-1, self.window_size * self.window_size]
            )
            attn_mask = tf.expand_dims(mask_windows, axis=1) - tf.expand_dims(
                mask_windows, axis=2
            )
            attn_mask = tf.where(attn_mask != 0, -100.0, attn_mask)
            attn_mask = tf.where(attn_mask == 0, 0.0, attn_mask)
            self.attn_mask = tf.Variable(initial_value=attn_mask, trainable=False)

    def call(self, x):
        height, width = self.num_patch
        _, num_patches_before, channels = x.shape
        x_skip = x
        x = self.norm1(x)
        x = tf.reshape(x, shape=(-1, height, width, channels))
        if self.shift_size > 0:
            shifted_x = tf.roll(
                x, shift=[-self.shift_size, -self.shift_size], axis=[1, 2]
            )
        else:
            shifted_x = x

        x_windows = window_partition(shifted_x, self.window_size)
        x_windows = tf.reshape(
            x_windows, shape=(-1, self.window_size * self.window_size, channels)
        )
        attn_windows = self.attn(x_windows, mask=self.attn_mask)

        attn_windows = tf.reshape(
            attn_windows, shape=(-1, self.window_size, self.window_size, channels)
        )
        shifted_x = window_reverse(
            attn_windows, self.window_size, height, width, channels
        )
        if self.shift_size > 0:
            x = tf.roll(
                shifted_x, shift=[self.shift_size, self.shift_size], axis=[1, 2]
            )
        else:
            x = shifted_x

        x = tf.reshape(x, shape=(-1, height * width, channels))
        x = self.drop_path(x)
        x = x_skip + x
        x_skip = x
        x = self.norm2(x)
        x = self.mlp(x)
        x = self.drop_path(x)
        x = x_skip + x
        return x
Model training and evaluation
Extract and embed patches
We first create 3 layers to help us extract, embed and merge patches from the images on top of which we will later use the Swin Transformer class we built.

class PatchExtract(layers.Layer):
    def __init__(self, patch_size, **kwargs):
        super(PatchExtract, self).__init__(**kwargs)
        self.patch_size_x = patch_size[0]
        self.patch_size_y = patch_size[0]

    def call(self, images):
        batch_size = tf.shape(images)[0]
        patches = tf.image.extract_patches(
            images=images,
            sizes=(1, self.patch_size_x, self.patch_size_y, 1),
            strides=(1, self.patch_size_x, self.patch_size_y, 1),
            rates=(1, 1, 1, 1),
            padding="VALID",
        )
        patch_dim = patches.shape[-1]
        patch_num = patches.shape[1]
        return tf.reshape(patches, (batch_size, patch_num * patch_num, patch_dim))


class PatchEmbedding(layers.Layer):
    def __init__(self, num_patch, embed_dim, **kwargs):
        super(PatchEmbedding, self).__init__(**kwargs)
        self.num_patch = num_patch
        self.proj = layers.Dense(embed_dim)
        self.pos_embed = layers.Embedding(input_dim=num_patch, output_dim=embed_dim)

    def call(self, patch):
        pos = tf.range(start=0, limit=self.num_patch, delta=1)
        return self.proj(patch) + self.pos_embed(pos)


class PatchMerging(tf.keras.layers.Layer):
    def __init__(self, num_patch, embed_dim):
        super(PatchMerging, self).__init__()
        self.num_patch = num_patch
        self.embed_dim = embed_dim
        self.linear_trans = layers.Dense(2 * embed_dim, use_bias=False)

    def call(self, x):
        height, width = self.num_patch
        _, _, C = x.get_shape().as_list()
        x = tf.reshape(x, shape=(-1, height, width, C))
        x0 = x[:, 0::2, 0::2, :]
        x1 = x[:, 1::2, 0::2, :]
        x2 = x[:, 0::2, 1::2, :]
        x3 = x[:, 1::2, 1::2, :]
        x = tf.concat((x0, x1, x2, x3), axis=-1)
        x = tf.reshape(x, shape=(-1, (height // 2) * (width // 2), 4 * C))
        return self.linear_trans(x)
Build the model
We put together the Swin Transformer model.

num_classes=4
input = layers.Input(input_shape)
x = layers.RandomCrop(image_dimension, image_dimension)(input)
x = layers.RandomFlip("horizontal")(x)
x = PatchExtract(patch_size)(x)
x = PatchEmbedding(num_patch_x * num_patch_y, embed_dim)(x)
x = SwinTransformer(
    dim=embed_dim,
    num_patch=(num_patch_x, num_patch_y),
    num_heads=num_heads,
    window_size=window_size,
    shift_size=0,
    num_mlp=num_mlp,
    qkv_bias=qkv_bias,
    dropout_rate=dropout_rate,
)(x)
x = SwinTransformer(
    dim=embed_dim,
    num_patch=(num_patch_x, num_patch_y),
    num_heads=num_heads,
    window_size=window_size,
    shift_size=shift_size,
    num_mlp=num_mlp,
    qkv_bias=qkv_bias,
    dropout_rate=dropout_rate,
)(x)
x = PatchMerging((num_patch_x, num_patch_y), embed_dim=embed_dim)(x)
x = layers.GlobalAveragePooling1D()(x)
output = layers.Dense(num_classes, activation="softmax")(x)
Train on CIFAR-100
We train the model on CIFAR-100. Here, we only train the model for 40 epochs to keep the training time short in this example. In practice, you should train for 150 epochs to reach convergence.

image_gen = ImageDataGenerator(preprocessing_function= tf.keras.applications.mobilenet_v2.preprocess_input)
train = image_gen.flow_from_dataframe(dataframe= train_set,x_col="filepaths",y_col="labels",
                                      target_size=(128,128),
                                      color_mode='rgb',
                                      class_mode="categorical", #used for Sequential Model
                                      batch_size=32,
                                      shuffle=False            #do not shuffle data
                                     )
test = image_gen.flow_from_dataframe(dataframe= test_images,x_col="filepaths", y_col="labels",
                                     target_size=(128,128),
                                     color_mode='rgb',
                                     class_mode="categorical",
                                     batch_size=32,
                                     shuffle= False
                                    )
val = image_gen.flow_from_dataframe(dataframe= val_set,x_col="filepaths", y_col="labels",
                                    target_size=(128,128),
                                    color_mode= 'rgb',
                                    class_mode="categorical",
                                    batch_size=32,
                                    shuffle=False
                                   )
Found 4569 validated image filenames belonging to 4 classes.
Found 857 validated image filenames belonging to 4 classes.
Found 1143 validated image filenames belonging to 4 classes.
model = keras.Model(input, output)
model.compile(
    loss=keras.losses.CategoricalCrossentropy(label_smoothing=label_smoothing),
    #optimizer=tfa.optimizers.AdamW(
    #    learning_rate=learning_rate, weight_decay=weight_decay
    #),
    optimizer=keras.optimizers.Adam(learning_rate=1e-3, decay=weight_decay),
    metrics=[
        keras.metrics.CategoricalAccuracy(name="accuracy"),
        keras.metrics.TopKCategoricalAccuracy(5, name="top-5-accuracy"),
    ],
)

history = model.fit(
    train,
    #y_train,
    #batch_size=128,
    epochs=num_epochs,
    #validation_split=validation_split,
    #validation_data=(x_test,y_test),
    validation_data=val,
)
Epoch 1/40
143/143 [==============================] - 23s 139ms/step - loss: 1.7990 - accuracy: 0.5303 - top-5-accuracy: 1.0000 - val_loss: 0.9461 - val_accuracy: 0.6964 - val_top-5-accuracy: 1.0000
Epoch 2/40
143/143 [==============================] - 18s 127ms/step - loss: 0.9113 - accuracy: 0.6982 - top-5-accuracy: 1.0000 - val_loss: 0.8647 - val_accuracy: 0.7332 - val_top-5-accuracy: 1.0000
Epoch 3/40
143/143 [==============================] - 19s 130ms/step - loss: 0.8584 - accuracy: 0.7339 - top-5-accuracy: 1.0000 - val_loss: 0.7997 - val_accuracy: 0.7647 - val_top-5-accuracy: 1.0000
Epoch 4/40
143/143 [==============================] - 19s 130ms/step - loss: 0.7590 - accuracy: 0.7897 - top-5-accuracy: 1.0000 - val_loss: 0.7245 - val_accuracy: 0.8206 - val_top-5-accuracy: 1.0000
Epoch 5/40
143/143 [==============================] - 18s 128ms/step - loss: 0.7127 - accuracy: 0.8183 - top-5-accuracy: 1.0000 - val_loss: 0.7623 - val_accuracy: 0.7857 - val_top-5-accuracy: 1.0000
Epoch 6/40
143/143 [==============================] - 18s 128ms/step - loss: 0.6822 - accuracy: 0.8378 - top-5-accuracy: 1.0000 - val_loss: 0.6834 - val_accuracy: 0.8259 - val_top-5-accuracy: 1.0000
Epoch 7/40
143/143 [==============================] - 19s 130ms/step - loss: 0.6285 - accuracy: 0.8693 - top-5-accuracy: 1.0000 - val_loss: 0.6560 - val_accuracy: 0.8408 - val_top-5-accuracy: 1.0000
Epoch 8/40
143/143 [==============================] - 19s 131ms/step - loss: 0.6022 - accuracy: 0.8860 - top-5-accuracy: 1.0000 - val_loss: 0.6602 - val_accuracy: 0.8609 - val_top-5-accuracy: 1.0000
Epoch 9/40
143/143 [==============================] - 18s 128ms/step - loss: 0.5889 - accuracy: 0.8963 - top-5-accuracy: 1.0000 - val_loss: 0.6658 - val_accuracy: 0.8635 - val_top-5-accuracy: 1.0000
Epoch 10/40
143/143 [==============================] - 19s 132ms/step - loss: 0.5757 - accuracy: 0.9103 - top-5-accuracy: 1.0000 - val_loss: 0.6237 - val_accuracy: 0.8836 - val_top-5-accuracy: 1.0000
Epoch 11/40
143/143 [==============================] - 19s 131ms/step - loss: 0.5412 - accuracy: 0.9227 - top-5-accuracy: 1.0000 - val_loss: 0.6525 - val_accuracy: 0.8513 - val_top-5-accuracy: 1.0000
Epoch 12/40
143/143 [==============================] - 18s 128ms/step - loss: 0.5180 - accuracy: 0.9361 - top-5-accuracy: 1.0000 - val_loss: 0.5857 - val_accuracy: 0.8880 - val_top-5-accuracy: 1.0000
Epoch 13/40
143/143 [==============================] - 20s 140ms/step - loss: 0.5232 - accuracy: 0.9352 - top-5-accuracy: 1.0000 - val_loss: 0.6131 - val_accuracy: 0.8801 - val_top-5-accuracy: 1.0000
Epoch 14/40
143/143 [==============================] - 21s 145ms/step - loss: 0.5028 - accuracy: 0.9448 - top-5-accuracy: 1.0000 - val_loss: 0.5986 - val_accuracy: 0.8880 - val_top-5-accuracy: 1.0000
Epoch 15/40
143/143 [==============================] - 21s 150ms/step - loss: 0.5005 - accuracy: 0.9477 - top-5-accuracy: 1.0000 - val_loss: 0.6036 - val_accuracy: 0.8889 - val_top-5-accuracy: 1.0000
Epoch 16/40
143/143 [==============================] - 21s 144ms/step - loss: 0.4874 - accuracy: 0.9584 - top-5-accuracy: 1.0000 - val_loss: 0.5868 - val_accuracy: 0.9003 - val_top-5-accuracy: 1.0000
Epoch 17/40
143/143 [==============================] - 21s 147ms/step - loss: 0.4774 - accuracy: 0.9637 - top-5-accuracy: 1.0000 - val_loss: 0.5682 - val_accuracy: 0.9064 - val_top-5-accuracy: 1.0000
Epoch 18/40
143/143 [==============================] - 20s 140ms/step - loss: 0.4623 - accuracy: 0.9724 - top-5-accuracy: 1.0000 - val_loss: 0.5592 - val_accuracy: 0.9099 - val_top-5-accuracy: 1.0000
Epoch 19/40
143/143 [==============================] - 18s 129ms/step - loss: 0.4598 - accuracy: 0.9718 - top-5-accuracy: 1.0000 - val_loss: 0.6380 - val_accuracy: 0.8810 - val_top-5-accuracy: 1.0000
Epoch 20/40
143/143 [==============================] - 19s 130ms/step - loss: 0.4545 - accuracy: 0.9744 - top-5-accuracy: 1.0000 - val_loss: 0.5483 - val_accuracy: 0.9213 - val_top-5-accuracy: 1.0000
Epoch 21/40
143/143 [==============================] - 19s 130ms/step - loss: 0.4492 - accuracy: 0.9779 - top-5-accuracy: 1.0000 - val_loss: 0.5788 - val_accuracy: 0.9160 - val_top-5-accuracy: 1.0000
Epoch 22/40
143/143 [==============================] - 18s 128ms/step - loss: 0.4383 - accuracy: 0.9849 - top-5-accuracy: 1.0000 - val_loss: 0.5352 - val_accuracy: 0.9160 - val_top-5-accuracy: 1.0000
Epoch 23/40
143/143 [==============================] - 18s 129ms/step - loss: 0.4371 - accuracy: 0.9825 - top-5-accuracy: 1.0000 - val_loss: 0.5575 - val_accuracy: 0.9116 - val_top-5-accuracy: 1.0000
Epoch 24/40
143/143 [==============================] - 18s 128ms/step - loss: 0.4345 - accuracy: 0.9812 - top-5-accuracy: 1.0000 - val_loss: 0.5568 - val_accuracy: 0.9160 - val_top-5-accuracy: 1.0000
Epoch 25/40
143/143 [==============================] - 19s 133ms/step - loss: 0.4324 - accuracy: 0.9838 - top-5-accuracy: 1.0000 - val_loss: 0.5435 - val_accuracy: 0.9143 - val_top-5-accuracy: 1.0000
Epoch 26/40
143/143 [==============================] - 18s 125ms/step - loss: 0.4373 - accuracy: 0.9829 - top-5-accuracy: 1.0000 - val_loss: 0.5400 - val_accuracy: 0.9221 - val_top-5-accuracy: 1.0000
Epoch 27/40
143/143 [==============================] - 19s 130ms/step - loss: 0.4238 - accuracy: 0.9880 - top-5-accuracy: 1.0000 - val_loss: 0.5213 - val_accuracy: 0.9291 - val_top-5-accuracy: 1.0000
Epoch 28/40
143/143 [==============================] - 19s 133ms/step - loss: 0.4269 - accuracy: 0.9862 - top-5-accuracy: 1.0000 - val_loss: 0.5627 - val_accuracy: 0.9186 - val_top-5-accuracy: 1.0000
Epoch 29/40
143/143 [==============================] - 18s 126ms/step - loss: 0.4197 - accuracy: 0.9886 - top-5-accuracy: 1.0000 - val_loss: 0.5614 - val_accuracy: 0.9125 - val_top-5-accuracy: 1.0000
Epoch 30/40
143/143 [==============================] - 19s 130ms/step - loss: 0.4253 - accuracy: 0.9866 - top-5-accuracy: 1.0000 - val_loss: 0.5570 - val_accuracy: 0.9055 - val_top-5-accuracy: 1.0000
Epoch 31/40
143/143 [==============================] - 19s 131ms/step - loss: 0.4119 - accuracy: 0.9912 - top-5-accuracy: 1.0000 - val_loss: 0.5276 - val_accuracy: 0.9178 - val_top-5-accuracy: 1.0000
Epoch 32/40
143/143 [==============================] - 19s 130ms/step - loss: 0.4120 - accuracy: 0.9915 - top-5-accuracy: 1.0000 - val_loss: 0.5331 - val_accuracy: 0.9204 - val_top-5-accuracy: 1.0000
Epoch 33/40
143/143 [==============================] - 18s 129ms/step - loss: 0.4125 - accuracy: 0.9917 - top-5-accuracy: 1.0000 - val_loss: 0.5304 - val_accuracy: 0.9221 - val_top-5-accuracy: 1.0000
Epoch 34/40
143/143 [==============================] - 19s 130ms/step - loss: 0.4103 - accuracy: 0.9926 - top-5-accuracy: 1.0000 - val_loss: 0.5168 - val_accuracy: 0.9265 - val_top-5-accuracy: 1.0000
Epoch 35/40
143/143 [==============================] - 19s 132ms/step - loss: 0.4168 - accuracy: 0.9893 - top-5-accuracy: 1.0000 - val_loss: 0.5247 - val_accuracy: 0.9291 - val_top-5-accuracy: 1.0000
Epoch 36/40
143/143 [==============================] - 18s 125ms/step - loss: 0.4128 - accuracy: 0.9923 - top-5-accuracy: 1.0000 - val_loss: 0.5104 - val_accuracy: 0.9318 - val_top-5-accuracy: 1.0000
Epoch 37/40
143/143 [==============================] - 19s 132ms/step - loss: 0.4067 - accuracy: 0.9932 - top-5-accuracy: 1.0000 - val_loss: 0.5138 - val_accuracy: 0.9335 - val_top-5-accuracy: 1.0000
Epoch 38/40
143/143 [==============================] - 18s 127ms/step - loss: 0.4022 - accuracy: 0.9952 - top-5-accuracy: 1.0000 - val_loss: 0.5137 - val_accuracy: 0.9300 - val_top-5-accuracy: 1.0000
Epoch 39/40
143/143 [==============================] - 18s 129ms/step - loss: 0.4052 - accuracy: 0.9923 - top-5-accuracy: 1.0000 - val_loss: 0.5389 - val_accuracy: 0.9116 - val_top-5-accuracy: 1.0000
Epoch 40/40
143/143 [==============================] - 18s 129ms/step - loss: 0.4046 - accuracy: 0.9934 - top-5-accuracy: 1.0000 - val_loss: 0.5716 - val_accuracy: 0.8941 - val_top-5-accuracy: 1.0000
model = keras.Model(input, output)
model.compile(
    loss=keras.losses.CategoricalCrossentropy(label_smoothing=label_smoothing),
    optimizer=tfa.optimizers.AdamW(
        learning_rate=learning_rate, weight_decay=weight_decay
    ),
    #optimizer=keras.optimizers.Adam(learning_rate=1e-2, decay=weight_decay),
    metrics=[
        keras.metrics.CategoricalAccuracy(name="accuracy"),
        keras.metrics.TopKCategoricalAccuracy(5, name="top-5-accuracy"),
    ],
)

history = model.fit(
    x_train,
    y_train,
    batch_size=256,
    epochs=num_epochs,
    #validation_split=validation_split,
    validation_data=(x_test,y_test),
    #validation_data=val,
)
Epoch 1/40
12/12 [==============================] - 5s 133ms/step - loss: 8.8973 - accuracy: 0.3122 - top-5-accuracy: 1.0000 - val_loss: 17.4221 - val_accuracy: 0.1954 - val_top-5-accuracy: 1.0000
Epoch 2/40
12/12 [==============================] - 1s 56ms/step - loss: 5.5362 - accuracy: 0.4028 - top-5-accuracy: 1.0000 - val_loss: 5.0962 - val_accuracy: 0.2487 - val_top-5-accuracy: 1.0000
Epoch 3/40
12/12 [==============================] - 1s 56ms/step - loss: 2.4793 - accuracy: 0.4659 - top-5-accuracy: 1.0000 - val_loss: 3.0304 - val_accuracy: 0.3579 - val_top-5-accuracy: 1.0000
Epoch 4/40
12/12 [==============================] - 1s 55ms/step - loss: 1.5946 - accuracy: 0.5582 - top-5-accuracy: 1.0000 - val_loss: 3.7525 - val_accuracy: 0.2843 - val_top-5-accuracy: 1.0000
Epoch 5/40
12/12 [==============================] - 1s 55ms/step - loss: 1.2578 - accuracy: 0.6010 - top-5-accuracy: 1.0000 - val_loss: 2.1842 - val_accuracy: 0.3985 - val_top-5-accuracy: 1.0000
Epoch 6/40
12/12 [==============================] - 1s 54ms/step - loss: 1.0496 - accuracy: 0.6505 - top-5-accuracy: 1.0000 - val_loss: 1.9753 - val_accuracy: 0.3756 - val_top-5-accuracy: 1.0000
Epoch 7/40
12/12 [==============================] - 1s 57ms/step - loss: 0.9932 - accuracy: 0.6610 - top-5-accuracy: 1.0000 - val_loss: 1.8133 - val_accuracy: 0.4695 - val_top-5-accuracy: 1.0000
Epoch 8/40
12/12 [==============================] - 1s 57ms/step - loss: 0.8991 - accuracy: 0.7052 - top-5-accuracy: 1.0000 - val_loss: 1.6897 - val_accuracy: 0.4467 - val_top-5-accuracy: 1.0000
Epoch 9/40
12/12 [==============================] - 1s 54ms/step - loss: 0.8614 - accuracy: 0.7310 - top-5-accuracy: 1.0000 - val_loss: 1.6612 - val_accuracy: 0.4949 - val_top-5-accuracy: 1.0000
Epoch 10/40
12/12 [==============================] - 1s 54ms/step - loss: 0.8560 - accuracy: 0.7293 - top-5-accuracy: 1.0000 - val_loss: 1.6636 - val_accuracy: 0.4721 - val_top-5-accuracy: 1.0000
Epoch 11/40
12/12 [==============================] - 1s 53ms/step - loss: 0.8431 - accuracy: 0.7328 - top-5-accuracy: 1.0000 - val_loss: 1.7686 - val_accuracy: 0.5076 - val_top-5-accuracy: 1.0000
Epoch 12/40
12/12 [==============================] - 1s 55ms/step - loss: 0.8213 - accuracy: 0.7613 - top-5-accuracy: 1.0000 - val_loss: 1.5907 - val_accuracy: 0.5330 - val_top-5-accuracy: 1.0000
Epoch 13/40
12/12 [==============================] - 1s 55ms/step - loss: 0.8109 - accuracy: 0.7571 - top-5-accuracy: 1.0000 - val_loss: 1.5327 - val_accuracy: 0.5305 - val_top-5-accuracy: 1.0000
Epoch 14/40
12/12 [==============================] - 1s 55ms/step - loss: 0.8311 - accuracy: 0.7467 - top-5-accuracy: 1.0000 - val_loss: 1.7037 - val_accuracy: 0.4416 - val_top-5-accuracy: 1.0000
Epoch 15/40
12/12 [==============================] - 1s 69ms/step - loss: 0.9097 - accuracy: 0.7080 - top-5-accuracy: 1.0000 - val_loss: 2.3791 - val_accuracy: 0.4010 - val_top-5-accuracy: 1.0000
Epoch 16/40
12/12 [==============================] - 1s 86ms/step - loss: 0.8907 - accuracy: 0.7244 - top-5-accuracy: 1.0000 - val_loss: 1.7529 - val_accuracy: 0.4594 - val_top-5-accuracy: 1.0000
Epoch 17/40
12/12 [==============================] - 1s 57ms/step - loss: 0.7990 - accuracy: 0.7620 - top-5-accuracy: 1.0000 - val_loss: 1.6415 - val_accuracy: 0.5152 - val_top-5-accuracy: 1.0000
Epoch 18/40
12/12 [==============================] - 1s 54ms/step - loss: 0.7902 - accuracy: 0.7725 - top-5-accuracy: 1.0000 - val_loss: 1.6806 - val_accuracy: 0.5609 - val_top-5-accuracy: 1.0000
Epoch 19/40
12/12 [==============================] - 1s 57ms/step - loss: 0.7371 - accuracy: 0.8080 - top-5-accuracy: 1.0000 - val_loss: 1.6457 - val_accuracy: 0.5635 - val_top-5-accuracy: 1.0000
Epoch 20/40
12/12 [==============================] - 1s 54ms/step - loss: 0.7341 - accuracy: 0.8115 - top-5-accuracy: 1.0000 - val_loss: 1.5580 - val_accuracy: 0.4898 - val_top-5-accuracy: 1.0000
Epoch 21/40
12/12 [==============================] - 1s 53ms/step - loss: 0.7264 - accuracy: 0.8098 - top-5-accuracy: 1.0000 - val_loss: 1.6665 - val_accuracy: 0.5381 - val_top-5-accuracy: 1.0000
Epoch 22/40
12/12 [==============================] - 1s 61ms/step - loss: 0.7018 - accuracy: 0.8209 - top-5-accuracy: 1.0000 - val_loss: 1.7306 - val_accuracy: 0.5558 - val_top-5-accuracy: 1.0000
Epoch 23/40
12/12 [==============================] - 1s 54ms/step - loss: 0.6938 - accuracy: 0.8352 - top-5-accuracy: 1.0000 - val_loss: 1.6426 - val_accuracy: 0.5635 - val_top-5-accuracy: 1.0000
Epoch 24/40
12/12 [==============================] - 1s 53ms/step - loss: 0.6985 - accuracy: 0.8376 - top-5-accuracy: 1.0000 - val_loss: 1.5822 - val_accuracy: 0.5736 - val_top-5-accuracy: 1.0000
Epoch 25/40
12/12 [==============================] - 1s 55ms/step - loss: 0.6855 - accuracy: 0.8334 - top-5-accuracy: 1.0000 - val_loss: 1.6885 - val_accuracy: 0.5888 - val_top-5-accuracy: 1.0000
Epoch 26/40
12/12 [==============================] - 1s 56ms/step - loss: 0.6985 - accuracy: 0.8317 - top-5-accuracy: 1.0000 - val_loss: 1.5195 - val_accuracy: 0.5102 - val_top-5-accuracy: 1.0000
Epoch 27/40
12/12 [==============================] - 1s 54ms/step - loss: 0.6950 - accuracy: 0.8376 - top-5-accuracy: 1.0000 - val_loss: 1.6086 - val_accuracy: 0.5736 - val_top-5-accuracy: 1.0000
Epoch 28/40
12/12 [==============================] - 1s 54ms/step - loss: 0.6899 - accuracy: 0.8296 - top-5-accuracy: 1.0000 - val_loss: 1.6654 - val_accuracy: 0.6193 - val_top-5-accuracy: 1.0000
Epoch 29/40
12/12 [==============================] - 1s 55ms/step - loss: 0.6976 - accuracy: 0.8286 - top-5-accuracy: 1.0000 - val_loss: 1.8268 - val_accuracy: 0.4975 - val_top-5-accuracy: 1.0000
Epoch 30/40
12/12 [==============================] - 1s 54ms/step - loss: 0.7030 - accuracy: 0.8240 - top-5-accuracy: 1.0000 - val_loss: 1.5291 - val_accuracy: 0.5964 - val_top-5-accuracy: 1.0000
Epoch 31/40
12/12 [==============================] - 1s 54ms/step - loss: 0.6802 - accuracy: 0.8334 - top-5-accuracy: 1.0000 - val_loss: 1.6160 - val_accuracy: 0.6117 - val_top-5-accuracy: 1.0000
Epoch 32/40
12/12 [==============================] - 1s 54ms/step - loss: 0.6419 - accuracy: 0.8624 - top-5-accuracy: 1.0000 - val_loss: 1.5112 - val_accuracy: 0.6218 - val_top-5-accuracy: 1.0000
Epoch 33/40
12/12 [==============================] - 1s 55ms/step - loss: 0.6544 - accuracy: 0.8526 - top-5-accuracy: 1.0000 - val_loss: 1.7533 - val_accuracy: 0.5888 - val_top-5-accuracy: 1.0000
Epoch 34/40
12/12 [==============================] - 1s 57ms/step - loss: 0.6364 - accuracy: 0.8662 - top-5-accuracy: 1.0000 - val_loss: 1.7655 - val_accuracy: 0.5660 - val_top-5-accuracy: 1.0000
Epoch 35/40
12/12 [==============================] - 1s 56ms/step - loss: 0.6411 - accuracy: 0.8620 - top-5-accuracy: 1.0000 - val_loss: 1.7630 - val_accuracy: 0.5533 - val_top-5-accuracy: 1.0000
Epoch 36/40
12/12 [==============================] - 1s 56ms/step - loss: 0.6447 - accuracy: 0.8655 - top-5-accuracy: 1.0000 - val_loss: 1.5666 - val_accuracy: 0.5761 - val_top-5-accuracy: 1.0000
Epoch 37/40
12/12 [==============================] - 1s 61ms/step - loss: 0.6237 - accuracy: 0.8770 - top-5-accuracy: 1.0000 - val_loss: 1.5148 - val_accuracy: 0.6574 - val_top-5-accuracy: 1.0000
Epoch 38/40
12/12 [==============================] - 1s 56ms/step - loss: 0.6018 - accuracy: 0.8923 - top-5-accuracy: 1.0000 - val_loss: 1.6097 - val_accuracy: 0.6371 - val_top-5-accuracy: 1.0000
Epoch 39/40
12/12 [==============================] - 1s 54ms/step - loss: 0.6229 - accuracy: 0.8770 - top-5-accuracy: 1.0000 - val_loss: 1.5191 - val_accuracy: 0.5990 - val_top-5-accuracy: 1.0000
Epoch 40/40
12/12 [==============================] - 1s 50ms/step - loss: 0.5970 - accuracy: 0.8916 - top-5-accuracy: 1.0000 - val_loss: 1.3899 - val_accuracy: 0.6421 - val_top-5-accuracy: 1.0000
Let's visualize the training progress of the model.

fig, ax = plt.subplots(figsize=(10, 6))

# Plot the data
ax.plot(history.history["accuracy"], label="Training Accuracy")
ax.plot(history.history["val_accuracy"], label="Validation Accuracy")

# Set the plot title and axis labels
#ax.set_title('ARIMA Performance')
ax.set_xlabel('Epochs')
ax.set_ylabel('Accuracy')

# Remove the grid lines
ax.grid(False)

# Set the legend
ax.legend(loc='lower right')
ax.set_facecolor('white')
ax.xaxis.label.set_color('black')
ax.yaxis.label.set_color('black')

# Set the color of tick labels to black
ax.tick_params(axis='x', colors='black')
ax.tick_params(axis='y', colors='black')
#fig.set_facecolor('white')
# Show the plot
plt.show()
fig, ax = plt.subplots(figsize=(10, 6))

# Plot the data
ax.plot(history.history["loss"], label="Training Loss")
ax.plot(history.history["val_loss"], label="Validation Loss")

# Set the plot title and axis labels
#ax.set_title('ARIMA Performance')
ax.set_xlabel('Epochs')
ax.set_ylabel('Loss')

# Remove the grid lines
ax.grid(False)

# Set the legend
ax.legend(loc='upper right')
ax.set_facecolor('white')
ax.xaxis.label.set_color('black')
ax.yaxis.label.set_color('black')

# Set the color of tick labels to black
ax.tick_params(axis='x', colors='black')
ax.tick_params(axis='y', colors='black')
#fig.set_facecolor('white')
# Show the plot
plt.show()
Let's display the final results of the training on CIFAR-100.

loss, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)
#loss, accuracy, top_5_accuracy = model.evaluate(x_test)
print(f"Test loss: {round(loss, 2)}")
print(f"Test accuracy: {round(accuracy * 100, 2)}%")
print(f"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%")
pred = model.predict(test)
#pred = np.argmax(pred, axis=1) #pick class with highest  probability

#pred
vit_y_pred = [np.argmax(probas) for probas in pred]
pred = model.predict(test)
pred = np.argmax(pred, axis=1) #pick class with highest  probability
labels = (train.class_indices)
labels = dict((v,k) for k,v in labels.items())
pred2 = [labels[k] for k in pred]
from sklearn.metrics import classification_report,accuracy_score
pred = model.predict(test)
pred = np.argmax(pred, axis=1) #pick class with highest  probability

labels = (train.class_indices)
labels = dict((v,k) for k,v in labels.items())
pred2 = [labels[k] for k in pred]
from sklearn.metrics import classification_report,accuracy_score

y_test = test_images.labels # set y_test to the expected output
print(classification_report(y_test, pred2))
print("Accuracy of the Model:",accuracy_score(y_test, pred2)*100,"%")
classification_report.strip()
#from sklearn.metrics import classification_report
report_lines = classification_report.strip().split('\n')
class_names = []
precisions = []
recalls = []
supports = []

for line in report_lines[2:-5]:  # Exclude headers and footer lines
    parts = line.split()
    class_names.append(parts[0])
    precisions.append(float(parts[1]))
    recalls.append(float(parts[2]))
    supports.append(int(parts[-1]))

# Calculate TP, FP, TN, FN for each class
TP = [int(recalls[i] * supports[i]) for i in range(len(class_names))]
FN = [supports[i] - TP[i] for i in range(len(class_names))]
FP = [int((TP[i] / precisions[i]) - TP[i]) for i in range(len(class_names))]
TN = [sum(supports) - (TP[i] + FP[i] + FN[i]) for i in range(len(class_names))]

# Print the results
for i in range(len(class_names)):
    print(f'Class: {class_names[i]} - TP: {TP[i]}, FP: {FP[i]}, TN: {TN[i]}, FN: {FN[i]}')
from sklearn.metrics import accuracy_score, multilabel_confusion_matrix

# Assuming your model is called 'model' and you have the test data 'x_test' and 'y_test'
predictions = model.predict(test)
predicted_labels = (predictions > 0.5).astype(int)  # Thresholding predicted probabilities

# Calculate accuracy
accuracy = accuracy_score(y_test, predicted_labels)
y_test = test_images.labels 
# Calculate multilabel confusion matrix
confusion_matrix = multilabel_confusion_matrix(y_test, predicted_labels)
print(f"Test accuracy: {round(accuracy * 100, 2)}%")
print("Multilabel Confusion Matrix:")
print(confusion_matrix)
from sklearn.metrics import classification_report

# Assuming your model is called 'model' and you have the test data 'x_test' and 'y_test'
predictions = model.predict(test)
predicted_labels = (predictions > 0.5).astype(int)  # Thresholding predicted probabilities

# Reshape the predicted_labels to match the shape of y_test
predicted_labels = predicted_labels.reshape(y_test.shape)

# Generate classification report
classification_report = classification_report(y_test, predicted_labels)
print(classification_report)
from sklearn.metrics import multilabel_confusion_matrix

# Assuming your model is called 'model' and you have the test data 'x_test' and 'y_test'
predictions = model.predict(x_test)
predicted_labels = (predictions > 0.5).astype(int)  # Thresholding predicted probabilities

# Reshape the predicted_labels to match the shape of y_test
predicted_labels = predicted_labels.reshape(y_test.shape)

# Calculate multilabel confusion matrix
confusion_matrix = multilabel_confusion_matrix(y_test, predicted_labels)
print("Multilabel Confusion Matrix:")
for i, matrix in enumerate(confusion_matrix):
    print(f"Class {i + 1}:")
    print(matrix)
import numpy as np
from sklearn.metrics import multilabel_confusion_matrix

# Assuming your model is called 'model' and you have the test data 'x_test' and 'y_test'
predictions = model.predict(x_test)
predicted_labels = (predictions > 0.5).astype(int)  # Thresholding predicted probabilities

# Reshape the predicted_labels to match the shape of y_test
predicted_labels = predicted_labels.reshape(y_test.shape)

# Calculate multilabel confusion matrix
confusion_matrix = multilabel_confusion_matrix(y_test, predicted_labels)

# Aggregate the individual confusion matrices into a single 4x4 confusion matrix
overall_confusion_matrix = np.vstack((
    np.hstack((confusion_matrix[0], confusion_matrix[1])),
    np.hstack((confusion_matrix[2], confusion_matrix[3]))
))

print("Overall Confusion Matrix:")
print(overall_confusion_matrix)
from sklearn.metrics import accuracy_score, confusion_matrix

# Assuming your model is called 'model' and you have the test data 'x_test' and 'y_test'
predictions = model.predict(x_test)
predicted_labels = np.argmax(predictions, axis=1)  # Get the predicted labels

# Calculate accuracy
accuracy = accuracy_score(y_test, predicted_labels)

# Calculate confusion matrix
confusion_matrix = confusion_matrix(y_test, predicted_labels)
print(f"Test accuracy: {round(accuracy * 100, 2)}%")
print("Confusion Matrix:")
print(confusion_matrix)
#labels = (x_train.class_indices)
labels = dict((v,k) for k,v in class_names_label.items())
pred2 = [labels[k] for k in pred]
y_test.labels
from sklearn.metrics import classification_report,accuracy_score

#y_test = test_images.labels # set y_test to the expected output
print(classification_report(y_test, vit_y_pred))
print("Accuracy of the Model:",accuracy_score(y_test, pred2)*100,"%")
cnf_matrix = confusion_matrix(y_test, vit_y_pred)
The Swin Transformer model we just trained has just 152K parameters, and it gets us to ~75% test top-5 accuracy within just 40 epochs without any signs of overfitting as well as seen in above graph. This means we can train this network for longer (perhaps with a bit more regularization) and obtain even better performance. This performance can further be improved by additional techniques like cosine decay learning rate schedule, other data augmentation techniques. While experimenting, I tried training the model for 150 epochs with a slightly higher dropout and greater embedding dimensions which pushes the performance to ~72% test accuracy on CIFAR-100 as you can see in the screenshot.

Results of training for longer

The authors present a top-1 accuracy of 87.3% on ImageNet. The authors also present a number of experiments to study how input sizes, optimizers etc. affect the final performance of this model. The authors further present using this model for object detection, semantic segmentation and instance segmentation as well and report competitive results for these. You are strongly advised to also check out the original paper.

This example takes inspiration from the official PyTorch and TensorFlow implementations.

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

input_shape = (128, 128, 3)  # Specify the desired input shape

# Swin Transformer block
class SwinTransformerBlock(layers.Layer):
    def __init__(self, embed_dim, num_heads, mlp_dim, qkv_bias, dropout_rate=0.0):
        super(SwinTransformerBlock, self).__init__()

        self.mlp_dim = mlp_dim
        self.qkv_bias = qkv_bias

        self.att = layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=embed_dim, dropout=dropout_rate
        )
        self.mlp = keras.Sequential(
            [
                layers.Dense(units=mlp_dim, activation=keras.activations.gelu),
                layers.Dropout(rate=dropout_rate),
                layers.Dense(units=embed_dim),
                layers.Dropout(rate=dropout_rate),
            ]
        )
        self.layernorm1 = layers.LayerNormalization(epsilon=1e-5)
        self.layernorm2 = layers.LayerNormalization(epsilon=1e-5)
        self.dropout1 = layers.Dropout(rate=dropout_rate)
        self.dropout2 = layers.Dropout(rate=dropout_rate)

    def call(self, inputs, training=False):
        # Cross-attention
        x = inputs
        x = self.layernorm1(x)
        attention_output = self.att(x, x, x)
        attention_output = self.dropout1(attention_output, training=training)
        out1 = x + attention_output

        # MLP
        x = out1
        x = self.layernorm2(x)
        x = self.mlp(x)
        x = self.dropout2(x, training=training)
        out2 = out1 + x

        return out2

# Swin Transformer model
class SwinTransformer(keras.Model):
    def __init__(
        self,
        input_shape=input_shape,
        patch_size=(2, 2),
        num_heads=4,
        embed_dim=64,
        num_mlp_layers=2,
        mlp_dim=256,
        qkv_bias=True,
        dropout_rate=0.0,
        num_classes=1000,
    ):
        super(SwinTransformer, self).__init__()

        self.num_classes = num_classes
        self.patch_size = patch_size
        self.embed_dim = embed_dim

        num_patches = (input_shape[0] // patch_size[0]) * (input_shape[1] // patch_size[1])
        self.patch_proj = layers.Conv2D(embed_dim, patch_size, strides=patch_size, padding="valid")
        self.pos_emb = self.add_weight(
            "pos_emb", shape=(1, num_patches + 1, embed_dim), initializer=keras.initializers.RandomNormal(), trainable=True
        )
        self.dropout = layers.Dropout(rate=dropout_rate)
        self.blocks = [
            SwinTransformerBlock(
                embed_dim=embed_dim,
                num_heads=num_heads,
                mlp_dim=mlp_dim,
                qkv_bias=qkv_bias,
                dropout_rate=dropout_rate,
            )
            for _ in range(num_mlp_layers)
        ]
        self.layernorm = layers.LayerNormalization(epsilon=1e-5)
        self.mlp_head = keras.Sequential(
            [
                layers.Dense(units=mlp_dim, activation=keras.activations.gelu),
                layers.Dropout(rate=dropout_rate),
                layers.Dense(units=num_classes),
            ]
        )

    def call(self, inputs, training=False):
        # Patch projection
        x = self.patch_proj(inputs)
        x = tf.reshape(x, shape=(-1, x.shape[1] * x.shape[2], x.shape[3]))

        # Positional embedding
        x = x + self.pos_emb

        # Dropout
        x = self.dropout(x, training=training)

        # Transformer blocks
        for block in self.blocks:
            x = block(x, training=training)

        # Layer normalization
        x = self.layernorm(x)

        # MLP head
        x = tf.reduce_mean(x, axis=1)
        x = self.mlp_head(x)

        return x

# Create the Swin Transformer model
model = SwinTransformer()

# Print model summary
model.build(input_shape=(None, *input_shape))
model.summary()
!pip install timm
Collecting timm
  Downloading timm-0.9.5-py3-none-any.whl (2.2 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.2/2.2 MB 20.1 MB/s eta 0:00:0000:0100:01
Requirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from timm) (0.10.1)
Requirement already satisfied: torch>=1.7 in /opt/conda/lib/python3.7/site-packages (from timm) (1.9.1)
Collecting safetensors
  Downloading safetensors-0.3.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 54.4 MB/s eta 0:00:00
Requirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from timm) (6.0)
Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.7/site-packages (from timm) (0.5.1)
Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.7->timm) (4.1.1)
Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm) (2.27.1)
Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm) (21.3)
Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm) (4.63.0)
Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm) (3.6.0)
Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm) (4.11.3)
Requirement already satisfied: pillow>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->timm) (9.0.1)
Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision->timm) (1.21.5)
Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.9->huggingface-hub->timm) (3.0.7)
Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->huggingface-hub->timm) (3.7.0)
Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub->timm) (2021.10.8)
Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub->timm) (2.0.12)
Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub->timm) (3.3)
Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub->timm) (1.26.8)
Installing collected packages: safetensors, timm
Successfully installed safetensors-0.3.3 timm-0.9.5
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv
import torchvision
import torch
from PIL import Image
import os
import numpy as np
import matplotlib.pyplot as plt
import random
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn
import torchvision.transforms as transforms
import cv2
import glob
import math
import timm
from PIL import ImageFilter
#from einops import rearrange
from timm.loss import LabelSmoothingCrossEntropy
timm.list_models('swin*', pretrained=True)
['swin_base_patch4_window7_224.ms_in1k',
 'swin_base_patch4_window7_224.ms_in22k',
 'swin_base_patch4_window7_224.ms_in22k_ft_in1k',
 'swin_base_patch4_window12_384.ms_in1k',
 'swin_base_patch4_window12_384.ms_in22k',
 'swin_base_patch4_window12_384.ms_in22k_ft_in1k',
 'swin_large_patch4_window7_224.ms_in22k',
 'swin_large_patch4_window7_224.ms_in22k_ft_in1k',
 'swin_large_patch4_window12_384.ms_in22k',
 'swin_large_patch4_window12_384.ms_in22k_ft_in1k',
 'swin_s3_base_224.ms_in1k',
 'swin_s3_small_224.ms_in1k',
 'swin_s3_tiny_224.ms_in1k',
 'swin_small_patch4_window7_224.ms_in1k',
 'swin_small_patch4_window7_224.ms_in22k',
 'swin_small_patch4_window7_224.ms_in22k_ft_in1k',
 'swin_tiny_patch4_window7_224.ms_in1k',
 'swin_tiny_patch4_window7_224.ms_in22k',
 'swin_tiny_patch4_window7_224.ms_in22k_ft_in1k',
 'swinv2_base_window8_256.ms_in1k',
 'swinv2_base_window12_192.ms_in22k',
 'swinv2_base_window12to16_192to256.ms_in22k_ft_in1k',
 'swinv2_base_window12to24_192to384.ms_in22k_ft_in1k',
 'swinv2_base_window16_256.ms_in1k',
 'swinv2_cr_small_224.sw_in1k',
 'swinv2_cr_small_ns_224.sw_in1k',
 'swinv2_cr_tiny_ns_224.sw_in1k',
 'swinv2_large_window12_192.ms_in22k',
 'swinv2_large_window12to16_192to256.ms_in22k_ft_in1k',
 'swinv2_large_window12to24_192to384.ms_in22k_ft_in1k',
 'swinv2_small_window8_256.ms_in1k',
 'swinv2_small_window16_256.ms_in1k',
 'swinv2_tiny_window8_256.ms_in1k',
 'swinv2_tiny_window16_256.ms_in1k']
!pip install huggingface_hub
Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.7/site-packages (0.5.1)
Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from huggingface_hub) (4.63.0)
Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.7/site-packages (from huggingface_hub) (21.3)
Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from huggingface_hub) (4.11.3)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface_hub) (4.1.1)
Requirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from huggingface_hub) (6.0)
Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from huggingface_hub) (2.27.1)
Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface_hub) (3.6.0)
Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.9->huggingface_hub) (3.0.7)
Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->huggingface_hub) (3.7.0)
Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface_hub) (3.3)
Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface_hub) (1.26.8)
Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface_hub) (2.0.12)
Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface_hub) (2021.10.8)
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv
model = timm.create_model('swin_base_patch4_window7_224', pretrained=True)
model.head = nn.Sequential(
            nn.Linear(1024, 512),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, class_len)) # Modify head according to this task

model = model.to(device)

criterion = LabelSmoothingCrossEntropy() # this is better than nn.CrossEntropyLoss
criterion = criterion.to(device)

optimizer = torch.optim.AdamW(model.head.parameters(), lr=lr) # Setting for transfer learning
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
/tmp/ipykernel_17/2075558178.py in <module>
----> 1 model = timm.create_model('swin_base_patch4_window7_224', pretrained=True)
      2 model.head = nn.Sequential(
      3             nn.Linear(1024, 512),
      4             nn.ReLU(),
      5             nn.Dropout(0.3),

/opt/conda/lib/python3.7/site-packages/timm/models/_factory.py in create_model(model_name, pretrained, pretrained_cfg, pretrained_cfg_overlay, checkpoint_path, scriptable, exportable, no_jit, **kwargs)
    116             pretrained_cfg=pretrained_cfg,
    117             pretrained_cfg_overlay=pretrained_cfg_overlay,
--> 118             **kwargs,
    119         )
    120 

/opt/conda/lib/python3.7/site-packages/timm/models/swin_transformer.py in swin_base_patch4_window7_224(pretrained, **kwargs)
    735     model_args = dict(patch_size=4, window_size=7, embed_dim=128, depths=(2, 2, 18, 2), num_heads=(4, 8, 16, 32))
    736     return _create_swin_transformer(
--> 737         'swin_base_patch4_window7_224', pretrained=pretrained, **dict(model_args, **kwargs))
    738 
    739 

/opt/conda/lib/python3.7/site-packages/timm/models/swin_transformer.py in _create_swin_transformer(variant, pretrained, **kwargs)
    620         pretrained_filter_fn=checkpoint_filter_fn,
    621         feature_cfg=dict(flatten_sequential=True, out_indices=out_indices),
--> 622         **kwargs)
    623 
    624     return model

/opt/conda/lib/python3.7/site-packages/timm/models/_builder.py in build_model_with_cfg(model_cls, variant, pretrained, pretrained_cfg, pretrained_cfg_overlay, model_cfg, feature_cfg, pretrained_strict, pretrained_filter_fn, kwargs_filter, **kwargs)
    397             in_chans=kwargs.get('in_chans', 3),
    398             filter_fn=pretrained_filter_fn,
--> 399             strict=pretrained_strict,
    400         )
    401 

/opt/conda/lib/python3.7/site-packages/timm/models/_builder.py in load_pretrained(model, pretrained_cfg, num_classes, in_chans, filter_fn, strict)
    155         raise RuntimeError("Invalid pretrained config, cannot load weights. Use `pretrained=False` for random init.")
    156 
--> 157     load_from, pretrained_loc = _resolve_pretrained_source(pretrained_cfg)
    158     if load_from == 'state_dict':
    159         _logger.info(f'Loading pretrained weights from state dict')

/opt/conda/lib/python3.7/site-packages/timm/models/_builder.py in _resolve_pretrained_source(pretrained_cfg)
     60                 # prioritized old cached weights if exists and env var enabled
     61                 old_cache_valid = check_cached_file(pretrained_url) if pretrained_url else False
---> 62             if not old_cache_valid and hf_hub_id and has_hf_hub(necessary=True):
     63                 # hf-hub available as alternate weight source in default_cfg
     64                 load_from = 'hf-hub'

/opt/conda/lib/python3.7/site-packages/timm/models/_hub.py in has_hf_hub(necessary)
    111         # if no HF Hub module installed, and it is necessary to continue, raise error
    112         raise RuntimeError(
--> 113             'Hugging Face hub model specified but package not installed. Run `pip install huggingface_hub`.')
    114     return _has_hf_hub
    115 

RuntimeError: Hugging Face hub model specified but package not installed. Run `pip install huggingface_hub`.












