Steps for Code Implementation:
Step 1: Install Dependencies
pip install torch torchvision torchaudio transformers matplotlib numpy scikit-learn
Step 2: Import Libraries
import torch
import torch.nn as nnimport tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.callbacks import EarlyStopping


# Conditional Contrastive GAN (CGAN) Network Architecture
def build_cgan(input_shape, num_classes):
    # Generator
    generator = build_generator(input_shape, num_classes)

    # Discriminator
    discriminator = build_discriminator(input_shape, num_classes)

    # CGAN Model
    cgan_input = layers.Input(shape=input_shape, name='cgan_input')
    generated_output = generator(cgan_input)
    cgan_output = discriminator([cgan_input, generated_output])

    cgan = models.Model(inputs=cgan_input, outputs=[generated_output, cgan_output])

    return cgan

    # Implement the generator architecture with skip connections
def build_generator(input_shape, num_classes):
    # Encoder
    input_layer = layers.Input(shape=input_shape, name='generator_input')
    x = layers.Conv2D(64, (3, 3), padding='same', activation='relu')(input_layer)
    skip_connections = [x]

    # Downsample
    for filters in [128, 256, 512]:
        x = layers.Conv2D(filters, (3, 3), strides=(2, 2), padding='same', activation='relu')(x)
        skip_connections.append(x)

    # Decoder
    for filters, skip_connection in zip([512, 256, 128, 64], reversed(skip_connections)):
        x = layers.Conv2DTranspose(filters, (3, 3), strides=(2, 2), padding='same', activation='relu')(x)
        x = layers.Concatenate()([x, skip_connection])

    output_layer = layers.Conv2D(num_classes, (1, 1), activation='softmax', name='generator_output')(x)

    generator = models.Model(inputs=input_layer, outputs=output_layer, name='generator')
    return generator

    # Implement the discriminator architecture
def build_discriminator(input_shape, num_classes):
    input_layer = layers.Input(shape=input_shape, name='discriminator_input')

    x = input_layer

    # Replace the following with your discriminator architecture
    x = layers.Conv2D(64, (3, 3), strides=(2, 2), padding='same', activation='relu')(x)
    x = layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same', activation='relu')(x)
    x = layers.Conv2D(256, (3, 3), strides=(2, 2), padding='same', activation='relu')(x)

    # Global average pooling
    x = layers.GlobalAveragePooling2D()(x)

    # Fully connected layer for classification
    x = layers.Dense(num_classes, activation='softmax', name='discriminator_output')(x)

    discriminator = models.Model(inputs=input_layer, outputs=x, name='discriminator')
    return discriminator

# Class-specific Attention Module
def class_specific_attention(input_tensor, num_classes):
    # Depth-wise separable convolution
    x = layers.DepthwiseConv2D((3, 3), padding='same', activation=None)(input_tensor)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)

    # Global max pooling
    x = layers.GlobalMaxPooling2D()(x)

    # Fully connected layer for class scores
    class_scores = layers.Dense(num_classes, activation='softmax', name='class_scores')(x)

    # Reshape class scores to match the input shape
    class_scores = layers.Reshape((1, 1, num_classes))(class_scores)

    # Multiply input by class scores element-wise
    class_specific_attention = layers.Multiply(name='class_specific_attention')([input_tensor, class_scores])

    return class_specific_attention


# Region-level Rebalance Module
def region_level_rebalance(input_tensor, num_classes):
    # Auxiliary region classification module
    region_classification = layers.Conv2D(num_classes, (3, 3), padding='same', activation='softmax', name='region_classification')(input_tensor)

    # Extracted area (A) from the region classification
    extracted_area = layers.Activation('softmax', name='extracted_area')(region_classification)

    # Ground-truth of the region (A_y)
    ground_truth_region = layers.Input(shape=(None, None, num_classes), name='ground_truth_region')

    # Loss function for region rebalancing
    def region_rebalance_loss(y_true, y_pred):
        F_C = layers.Lambda(lambda x: layers.sum(layers.cast(layers.equal(x[0], x[1]), dtype='float32')))([ground_truth_region, extracted_area])
        A_y = layers.sum(y_true * extracted_area, axis=[1, 2])

        loss = -layers.log(A_y / layers.sum(F_C * layers.exp(y_pred), axis=[1, 2]))

        return loss

    # Pixel loss (Loss_pixel)
    pixel_loss = layers.Lambda(lambda x: -layers.log(x))(extracted_area)

    # Region loss (Loss_region)
    region_loss = layers.Lambda(region_rebalance_loss, name='region_loss')([ground_truth_region, region_classification])

    # Combined loss (Loss_all)
    combined_loss = layers.Add(name='loss_all')([pixel_loss, region_loss])

    return combined_loss

# Supervised Contrastive Learning-based Network (SCoLN)
def SCoLN(input_shape):
    # Input tensor
    input_tensor = layers.Input(shape=input_shape, name='input_tensor')

    # ResNet-50 architecture with a residual module
    resnet_base = tf.keras.applications.ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)
    resnet_base.trainable = False

    # Contrastive learning part
    positive_output = layers.Dense(128, activation='relu', name='positive_output')(resnet_base(input_tensor))
    negative_output = layers.Dense(128, activation='relu', name='negative_output')(resnet_base(input_tensor))

    # Contrastive loss
    def contrastive_loss(y_true, y_pred):
        margin = 1
        positive_sim = tf.exp(tf.reduce_sum(y_true * y_pred[0], axis=-1))
        negative_sim = tf.reduce_sum(tf.exp(tf.reduce_sum(y_true * y_pred[1], axis=-1)))
        loss = -tf.math.log(positive_sim / (positive_sim + negative_sim + 1e-10))
        return loss

    # Contrastive loss layer
    contrastive_loss_layer = layers.Lambda(contrastive_loss, name='contrastive_loss')([positive_output, negative_output])

    # Classification part
    classification_output = layers.Dense(1, activation='sigmoid', name='classification_output')(resnet_base(input_tensor))

    # Classification loss
    classification_loss = 'binary_crossentropy'

    # Build the model
    SCoLN_model = models.Model(inputs=input_tensor, outputs=[contrastive_loss_layer, classification_output], name='SCoLN')
    SCoLN_model.compile(optimizer='adam', loss={'contrastive_loss': contrastive_loss, 'classification_output': classification_loss})

    return SCoLN_model

# Evaluation Measure Functions
def intersection_over_union(TP, FP, FN):
    return 2 * TP / (2 * TP + FP + FN)

def dice_similarity_coefficient(TP, FP, FN):
    return TP / (TP + FP + FN)

def ppv (TP, FP):
    return TP / (TP + FP)

def npv (TP, FN):
    return TN / (TN + FN)

# Example usage
input_shape = (512, 512, 3)
num_classes = 3  # Adjust based on your task
cgan_model = build_cgan(input_shape, num_classes)

# Compile and train the model
SCoLN_model = SCoLN(input_shape)

SCoLN_model.compile(optimizer=tf.keras.optimizers.Adadelta(learning_rate=0.001, decay=1e-4),
                    loss={'contrastive_loss': contrastive_loss, 'classification_output': 'binary_crossentropy'},
                    metrics={'contrastive_loss': None, 'classification_output': 'accuracy'})

# Define early stopping callback
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Train the model
history = SCoLN_model.fit(datagen.flow(train_data, {'contrastive_loss': train_labels, 'classification_output': train_labels},
                                       batch_size=32, shuffle=True),
                          epochs=450,
                          steps_per_epoch=len(train_data) // 32,
                          validation_data=(val_data, {'contrastive_loss': val_labels, 'classification_output': val_labels}),
                          callbacks=[early_stopping])

# Plot training history
#plt.plot(history.history['contrastive_loss'], label='Contrastive Loss')
#plt.plot(history.history['classification_output_accuracy'], label='Classification Accuracy')
#plt.plot(history.history['val_contrastive_loss'], label='Validation Contrastive Loss')
#plt.plot(history.history['val_classification_output_accuracy'], label='Validation Classification Accuracy')
#plt.xlabel('Epoch')
#plt.ylabel('Loss/Accuracy')
#plt.legend()
#plt.show()

# Evaluate segmentation performance
TP, FP, FN = 100, 10, 5  # Example values, replace with actual values
iou = intersection_over_union(TP, FP, FN)
dsc = dice_similarity_coefficient(TP, FP, FN)
ppv = ppv (TP, FP)
npv = npv (TN, FN)

# Print or use evaluation metrics as needed
print("IoU:", iou)
print("DSC:", dsc)
print("ppv:", prec)
print("npv:", rec)
import torch.optim as optim
from torchvision import transforms
from torch.utils.data import DataLoader, Dataset
from transformers import SwinTransformer
import matplotlib.pyplot as plt
import numpy as np
Step 3: Data Preprocessing
Step 3.1: Load MRI Data
class BrainTumorDataset(Dataset):
    def __init__ (self, image_paths, mask_paths, transform=None):
        self.image_paths = image_paths
        self.mask_paths = mask_paths
        self.transform = transform

    def __len__(self):
        return len(self.image_paths)
    def __getitem__(self, idx):
        image = np.load(self.image_paths[idx])  # load MRI image
        mask = np.load(self.mask_paths[idx])    # load corresponding mask
        if self.transform:
            image = self.transform(image)
            mask = self.transform(mask)
        return image, mask
Step 3.2: Image Transformations
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Resize((256, 256)),
    transforms.Normalize(mean=[0.5], std=[0.5])])
    train_dataset=BrainTumorDataset(image_paths='path_to_images',      
    mask_paths='path_to_masks', transform=transform)

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
Step 4: Define U-Net Architecture
class UNet(nn.Module):
    def __init__(self, in_channels=1, out_channels=2):
        super(UNet, self).__init__()
        # Encoder
        self.enc1 = self.conv_block(in_channels, 64)
        self.enc2 = self.conv_block(64, 128)
        self.enc3 = self.conv_block(128, 256)
        self.enc4 = self.conv_block(256, 512)
        # Decoder
        self.dec1 = self.upconv_block(512, 256)
        self.dec2 = self.upconv_block(256, 128)
        self.dec3 = self.upconv_block(128, 64)
        self.dec4 = self.upconv_block(64, out_channels)
        self.pool = nn.MaxPool2d(2)
    def conv_block(self, in_channels, out_channels):
        return nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),
            nn.ReLU()
        )
    def upconv_block(self, in_channels, out_channels):
        return nn.Sequential(
            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),
            nn.ReLU()
        )
    def forward(self, x):
        # Encoder
        enc1 = self.enc1(x)
        enc2 = self.enc2(self.pool(enc1))
        enc3 = self.enc3(self.pool(enc2))
        enc4 = self.enc4(self.pool(enc3))
        # Decoder
        dec1 = self.dec1(enc4)
        dec2 = self.dec2(dec1 + enc3)
        dec3 = self.dec3(dec2 + enc2)
        dec4 = self.dec4(dec3 + enc1)
        return dec4
Step: 5 Define Class Conditional GAN (cGAN)
Step 5.1: Generator (U-Net)
class Generator(nn.Module):
    def __init__(self, in_channels=1, out_channels=2):
        super (Generator, self).__init__()
        self.unet = UNet(in_channels, out_channels)
    def forward (self, x):
        return self.unet(x)
Step 5.2: Discriminator
class Discriminator(nn.Module):
    def __init__(self, in_channels=3):  # Input channels: 1 (image) + 1 (condition)
        super(Discriminator, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=4, stride=2, padding=1)
        self.conv2 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)
        self.conv3 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)
        self.fc = nn.Linear(256 * 16 * 16, 1)
    def forward(self, x):
        x = torch.relu(self.conv1(x))
        x = torch.relu(self.conv2(x))
        x = torch.relu(self.conv3(x))
        x = x.view(x.size(0), -1)  # Flatten the output for the fully connected layer
        return torch.sigmoid(self.fc(x))
Step: 6 Integrating Swin Transformer
class SwinTransformerExtractor(nn.Module):
    def __init__(self):
        super (SwinTransformerExtractor, self).__init__()
        self.swin_transformer=SwinTransformer.from_pretrained('swin-base-patch4- 
        window7-224')
    def forward (self, x):
        return self.swin_transformer(x)
Step: 7Training the Model
def train(generator, discriminator, dataloader, num_epochs=50):
    criterion_gan = nn.BCELoss()  # Binary Cross Entropy Loss for GAN
    criterion_l1 = nn.L1Loss()  # L1 loss for pixel-wise reconstruction
    
    optimizer_g = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))
    optimizer_d = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))

    for epoch in range(num_epochs):
        for i, (images, masks) in enumerate(dataloader):
            # Create labels for real and fake images
            real_labels = torch.ones(images.size(0), 1)
            fake_labels = torch.zeros(images.size(0), 1)

            # Train the discriminator
            optimizer_d.zero_grad()
            real_images = torch.cat([images, masks], dim=1)
            outputs = discriminator(real_images)
            d_loss_real = criterion_gan(outputs, real_labels)

            fake_images = generator(images)
            fake_images_with_condition = torch.cat([images, fake_images], dim=1)
            outputs_fake = discriminator(fake_images_with_condition)
            d_loss_fake = criterion_gan(outputs_fake, fake_labels)

            d_loss = d_loss_real + d_loss_fake
            d_loss.backward()
            optimizer_d.step()

            # Train the generator
            optimizer_g.zero_grad()
            outputs = discriminator(fake_images_with_condition)
            g_loss_gan = criterion_gan(outputs, real_labels)

            g_loss_l1 = criterion_l1(fake_images, masks)
            g_loss = g_loss_gan + 100 * g_loss_l1
            g_loss.backward()
            optimizer_g.step()
        print(f"Epoch [{epoch+1}/{num_epochs}], D Loss: {d_loss.item()}, G Loss:  
         {g_loss.item()}")
Step: 8 Model Evaluation
def evaluate(generator, dataloader):
    generator.eval()
    with torch.no_grad():
        for images, masks in dataloader:
            output = generator(images)
            # Calculate Dice Similarity Coefficient, Accuracy, etc.
            dice_score = calculate_dice(output, masks)
            print(f"Dice Score: {dice_score}")
Step: 9 Visualization
def visualize_output(generator, dataloader):
    generator.eval()
    images, masks = next(iter(dataloader))
    output = generator(images)
    # Visualize
    plt.subplot(1, 3, 1)
    plt.imshow(images[0].squeeze(), cmap='gray')
    plt.title('Input Image')

    plt.subplot(1, 3, 2)
    plt.imshow(masks[0].squeeze(), cmap='gray')
    plt.title('Ground Truth')

    plt.subplot(1, 3, 3)
    plt.imshow(output[0].squeeze(), cmap='gray')
    plt.title('Predicted Output')
    plt.show()

