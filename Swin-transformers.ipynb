{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Image classification with Swin Transformers","metadata":{}},{"cell_type":"markdown","source":"This example implements [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030)\nby Liu et al. for image classification, and demonstrates it on the\n[CIFAR-100 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).\n\nSwin Transformer (**S**hifted **Win**dow Transformer) can serve as a general-purpose backbone\nfor computer vision. Swin Transformer is a hierarchical Transformer whose\nrepresentations are computed with _shifted windows_. The shifted window scheme\nbrings greater efficiency by limiting self-attention computation to\nnon-overlapping local windows while also allowing for cross-window connections.\nThis architecture has the flexibility to model information at various scales and has\na linear computational complexity with respect to image size.\n\nThis example requires TensorFlow 2.5 or higher, as well as TensorFlow Addons,\nwhich can be installed using the following commands:","metadata":{}},{"cell_type":"code","source":"!pip install -U tensorflow-addons","metadata":{"execution":{"iopub.status.busy":"2023-08-26T06:20:37.285317Z","iopub.execute_input":"2023-08-26T06:20:37.285989Z","iopub.status.idle":"2023-08-26T06:21:00.484309Z","shell.execute_reply.started":"2023-08-26T06:20:37.285898Z","shell.execute_reply":"2023-08-26T06:21:00.483352Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorflow-addons in /opt/conda/lib/python3.7/site-packages (0.14.0)\nCollecting tensorflow-addons\n  Downloading tensorflow_addons-0.19.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /opt/conda/lib/python3.7/site-packages (from tensorflow-addons) (2.13.3)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from tensorflow-addons) (21.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->tensorflow-addons) (3.0.7)\nInstalling collected packages: tensorflow-addons\n  Attempting uninstall: tensorflow-addons\n    Found existing installation: tensorflow-addons 0.14.0\n    Uninstalling tensorflow-addons-0.14.0:\n      Successfully uninstalled tensorflow-addons-0.14.0\nSuccessfully installed tensorflow-addons-0.19.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"## Setup","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport os","metadata":{"execution":{"iopub.status.busy":"2023-08-26T06:21:00.489963Z","iopub.execute_input":"2023-08-26T06:21:00.492050Z","iopub.status.idle":"2023-08-26T06:21:05.942628Z","shell.execute_reply.started":"2023-08-26T06:21:00.492004Z","shell.execute_reply":"2023-08-26T06:21:05.941826Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/tensorflow_addons/utils/ensure_tf_install.py:67: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.9.0 and strictly below 2.12.0 (nightly versions are not supported). \n The versions of TensorFlow you are currently using is 2.6.3 and is not supported. \nSome things might work, some things might not.\nIf you were to encounter a bug, do not file an issue.\nIf you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \nYou can find the compatibility matrix in TensorFlow Addon's readme:\nhttps://github.com/tensorflow/addons\n  UserWarning,\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport os\nfrom sklearn.metrics import confusion_matrix\n#import seaborn as sn; sn.set(font_scale=1.4)\nfrom sklearn.utils import shuffle           \nimport matplotlib.pyplot as plt             \nimport cv2                                 \nimport tensorflow as tf                \nfrom tqdm import tqdm\n#from sklearn.metrics import classification_report, log_loss, accuracy_score\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Conv2D, MaxPool2D, Flatten\nfrom keras.utils import np_utils\nimport tensorflow as tf\nimport datetime\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score","metadata":{"execution":{"iopub.status.busy":"2023-08-26T06:21:05.944032Z","iopub.execute_input":"2023-08-26T06:21:05.944317Z","iopub.status.idle":"2023-08-26T06:21:06.590815Z","shell.execute_reply.started":"2023-08-26T06:21:05.944255Z","shell.execute_reply":"2023-08-26T06:21:06.590004Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class_names = ['glioma_tumor','meningioma_tumor','no_tumor','pituitary_tumor']\nclass_names_label = {class_name:i for i, class_name in enumerate(class_names)}\n\nnb_classes = len(class_names)\n\nIMAGE_SIZE = (128, 128)","metadata":{"execution":{"iopub.status.busy":"2023-08-26T06:21:06.592655Z","iopub.execute_input":"2023-08-26T06:21:06.592923Z","iopub.status.idle":"2023-08-26T06:21:06.598665Z","shell.execute_reply.started":"2023-08-26T06:21:06.592892Z","shell.execute_reply":"2023-08-26T06:21:06.597976Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def load_data():\n    \"\"\"\n        Load the data:\n            - 14,034 images to train the network.\n            - 3,000 images to evaluate how accurately the network learned to classify images.\n    \"\"\"\n    TrainF=r\"/kaggle/input/brain-tumor-classification-mri/Training\"\n    TestF=r\"/kaggle/input/brain-tumor-classification-mri/Testing\"\n    datasets =  [TrainF,TestF]\n    output = []\n    \n    # Iterate through training and test sets\n    for dataset in datasets:\n        \n        images = []\n        labels = []\n        \n        print(\"Loading {}\".format(dataset))\n        \n        # Iterate through each folder corresponding to a category\n        for folder in os.listdir(dataset):\n            label = class_names_label[folder]\n            \n            # Iterate through each image in our folder\n            for file in tqdm(os.listdir(os.path.join(dataset, folder))):\n                \n                # Get the path name of the image\n                img_path = os.path.join(os.path.join(dataset, folder), file)\n                \n                # Open and resize the img\n                image = cv2.imread(img_path)\n                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n                image = cv2.resize(image, IMAGE_SIZE) \n                \n                # Append the image and its corresponding label to the output\n                images.append(image)\n                labels.append(label)\n                \n        images = np.array(images, dtype = 'float32')\n        labels = np.array(labels, dtype = 'int32')   \n        \n        output.append((images, labels))\n\n    return output","metadata":{"execution":{"iopub.status.busy":"2023-08-26T06:21:11.546217Z","iopub.execute_input":"2023-08-26T06:21:11.546837Z","iopub.status.idle":"2023-08-26T06:21:11.558631Z","shell.execute_reply.started":"2023-08-26T06:21:11.546794Z","shell.execute_reply":"2023-08-26T06:21:11.557468Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"num_classes=4\n(x_train, y_train), (x_test, y_test) = load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\nprint(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\nprint(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n\n#plt.figure(figsize=(10, 10))\n#for i in range(25):\n#    plt.subplot(5, 5, i + 1)\n#    plt.xticks([])\n#    plt.yticks([])\n#    plt.grid(False)\n#    plt.imshow(x_train[i])\n#plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-26T06:21:15.347191Z","iopub.execute_input":"2023-08-26T06:21:15.347812Z","iopub.status.idle":"2023-08-26T06:21:46.161356Z","shell.execute_reply.started":"2023-08-26T06:21:15.347772Z","shell.execute_reply":"2023-08-26T06:21:46.160505Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Loading /kaggle/input/brain-tumor-classification-mri/Training\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 395/395 [00:03<00:00, 117.00it/s]\n100%|██████████| 827/827 [00:08<00:00, 96.72it/s] \n100%|██████████| 822/822 [00:07<00:00, 115.96it/s]\n100%|██████████| 826/826 [00:07<00:00, 113.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loading /kaggle/input/brain-tumor-classification-mri/Testing\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 105/105 [00:00<00:00, 163.68it/s]\n100%|██████████| 74/74 [00:00<00:00, 78.32it/s]\n100%|██████████| 115/115 [00:00<00:00, 128.53it/s]\n100%|██████████| 100/100 [00:00<00:00, 110.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"x_train shape: (2870, 128, 128, 3) - y_train shape: (2870, 4)\nx_test shape: (394, 128, 128, 3) - y_test shape: (394, 4)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Configure the hyperparameters\n\nA key parameter to pick is the `patch_size`, the size of the input patches.\nIn order to use each pixel as an individual input, you can set `patch_size` to `(1, 1)`.\nBelow, we take inspiration from the original paper settings\nfor training on ImageNet-1K, keeping most of the original settings for this example.","metadata":{}},{"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2023-08-26T06:21:46.162952Z","iopub.execute_input":"2023-08-26T06:21:46.163399Z","iopub.status.idle":"2023-08-26T06:21:46.168044Z","shell.execute_reply.started":"2023-08-26T06:21:46.163358Z","shell.execute_reply":"2023-08-26T06:21:46.167356Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"tumor_dir=r'/kaggle/input/brain-tumor-mri-dataset/Training/glioma'\ntumor_dir1=r'/kaggle/input/brain-tumor-mri-dataset/Training/meningioma'\ntumor_dir2=r'/kaggle/input/brain-tumor-mri-dataset/Training/pituitary'\nhealthy_dir=r'/kaggle/input/brain-tumor-mri-dataset/Training/notumor'\nfilepaths = []\nlabels= []\ndict_list = [tumor_dir,tumor_dir1, tumor_dir2, healthy_dir]\nfor i, j in enumerate(dict_list):\n    flist=os.listdir(j)\n    for f in flist:\n        fpath=os.path.join(j,f)\n        filepaths.append(fpath)\n        if i==0:\n          labels.append('glioma_tumor')\n        elif i==1:\n          labels.append('meningioma_tumor')\n        elif i==2:\n          labels.append('pituitary_tumor')\n        else:\n          labels.append('no_tumor')\n    \nFseries = pd.Series(filepaths, name=\"filepaths\")\nLseries = pd.Series(labels, name=\"labels\")\ntumor_data = pd.concat([Fseries,Lseries], axis=1)\ntumor_df = pd.DataFrame(tumor_data)\nprint(tumor_df.head())\nprint(tumor_df[\"labels\"].value_counts())\n\n#shape of datatset\ntumor_df.shape","metadata":{"execution":{"iopub.status.busy":"2023-08-26T06:49:31.380287Z","iopub.execute_input":"2023-08-26T06:49:31.380609Z","iopub.status.idle":"2023-08-26T06:49:32.108725Z","shell.execute_reply.started":"2023-08-26T06:49:31.380574Z","shell.execute_reply":"2023-08-26T06:49:32.107427Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"                                           filepaths        labels\n0  /kaggle/input/brain-tumor-mri-dataset/Training...  glioma_tumor\n1  /kaggle/input/brain-tumor-mri-dataset/Training...  glioma_tumor\n2  /kaggle/input/brain-tumor-mri-dataset/Training...  glioma_tumor\n3  /kaggle/input/brain-tumor-mri-dataset/Training...  glioma_tumor\n4  /kaggle/input/brain-tumor-mri-dataset/Training...  glioma_tumor\nno_tumor            1595\npituitary_tumor     1457\nmeningioma_tumor    1339\nglioma_tumor        1321\nName: labels, dtype: int64\n","output_type":"stream"},{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"(5712, 2)"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split \ntrain_images, test_images = train_test_split(tumor_df, test_size=0.15, random_state=42)\ntrain_set, val_set = train_test_split(tumor_df, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-08-26T06:49:36.457933Z","iopub.execute_input":"2023-08-26T06:49:36.458216Z","iopub.status.idle":"2023-08-26T06:49:36.469743Z","shell.execute_reply.started":"2023-08-26T06:49:36.458183Z","shell.execute_reply":"2023-08-26T06:49:36.468856Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"input_shape=(128,128,3)","metadata":{"execution":{"iopub.status.busy":"2023-08-26T06:22:10.166070Z","iopub.execute_input":"2023-08-26T06:22:10.166373Z","iopub.status.idle":"2023-08-26T06:22:10.170614Z","shell.execute_reply.started":"2023-08-26T06:22:10.166335Z","shell.execute_reply":"2023-08-26T06:22:10.169518Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"patch_size = (32, 32)  # 2-by-2 sized patches\ndropout_rate = 0.03  # Dropout rate\nnum_heads =8  # Attention heads\nembed_dim = 64  # Embedding dimension\nnum_mlp = 256  # MLP layer size\nqkv_bias = True  # Convert embedded patches to query, key, and values with a learnable additive value\nwindow_size = 2  # Size of attention window\nshift_size = 1  # Size of shifting window\nimage_dimension = 128  # Initial image size\n\nnum_patch_x = input_shape[0] // patch_size[0]\nnum_patch_y = input_shape[1] // patch_size[1]\n\nlearning_rate = 1e-3\nbatch_size = 64\nnum_epochs = 40\nvalidation_split = 0.1\nweight_decay = 0.0001\nlabel_smoothing = 0.1","metadata":{"execution":{"iopub.status.busy":"2023-08-26T08:10:28.018148Z","iopub.execute_input":"2023-08-26T08:10:28.018754Z","iopub.status.idle":"2023-08-26T08:10:28.026323Z","shell.execute_reply.started":"2023-08-26T08:10:28.018716Z","shell.execute_reply":"2023-08-26T08:10:28.025419Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"patch_size = (10, 10)  # Larger patch size to capture more context\ndropout_rate = 0.2  # Slightly higher dropout rate for larger input\nnum_heads = 8  # Keep the same number of attention heads\nembed_dim = 128  # Increase embedding dimension for larger input\nnum_mlp = 512  # Increase MLP layer size for more complex data\nqkv_bias = True  # Keep the same\nwindow_size = 5  # Increase attention window size for larger context\nshift_size = 2  # Increase shifting window for more overlap\nimage_dimension = 150  # Update initial image size to match the new input\n\nnum_patch_x = input_shape[0] // patch_size[0]\nnum_patch_y = input_shape[1] // patch_size[1]\n\nlearning_rate = 1e-3\nbatch_size = 32  # Reduce batch size for larger input (adjust as needed)\nnum_epochs = 40\nvalidation_split = 0.1\nweight_decay = 0.0001\nlabel_smoothing = 0.1\n","metadata":{"execution":{"iopub.status.busy":"2023-08-25T07:07:12.511494Z","iopub.execute_input":"2023-08-25T07:07:12.511894Z","iopub.status.idle":"2023-08-25T07:07:12.525500Z","shell.execute_reply.started":"2023-08-25T07:07:12.511857Z","shell.execute_reply":"2023-08-25T07:07:12.524697Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Helper functions\n\nWe create two helper functions to help us get a sequence of\npatches from the image, merge patches, and apply dropout.","metadata":{}},{"cell_type":"code","source":"\ndef window_partition(x, window_size):\n    _, height, width, channels = x.shape\n    patch_num_y = height // window_size\n    patch_num_x = width // window_size\n    x = tf.reshape(\n        x, shape=(-1, patch_num_y, window_size, patch_num_x, window_size, channels)\n    )\n    x = tf.transpose(x, (0, 1, 3, 2, 4, 5))\n    windows = tf.reshape(x, shape=(-1, window_size, window_size, channels))\n    return windows\n\n\ndef window_reverse(windows, window_size, height, width, channels):\n    patch_num_y = height // window_size\n    patch_num_x = width // window_size\n    x = tf.reshape(\n        windows,\n        shape=(-1, patch_num_y, patch_num_x, window_size, window_size, channels),\n    )\n    x = tf.transpose(x, perm=(0, 1, 3, 2, 4, 5))\n    x = tf.reshape(x, shape=(-1, height, width, channels))\n    return x\n\n\nclass DropPath(layers.Layer):\n    def __init__(self, drop_prob=None, **kwargs):\n        super(DropPath, self).__init__(**kwargs)\n        self.drop_prob = drop_prob\n\n    def call(self, x):\n        input_shape = tf.shape(x)\n        batch_size = input_shape[0]\n        rank = x.shape.rank\n        shape = (batch_size,) + (1,) * (rank - 1)\n        random_tensor = (1 - self.drop_prob) + tf.random.uniform(shape, dtype=x.dtype)\n        path_mask = tf.floor(random_tensor)\n        output = tf.math.divide(x, 1 - self.drop_prob) * path_mask\n        return output\n","metadata":{"execution":{"iopub.status.busy":"2023-08-26T06:22:34.869798Z","iopub.execute_input":"2023-08-26T06:22:34.870130Z","iopub.status.idle":"2023-08-26T06:22:34.891242Z","shell.execute_reply.started":"2023-08-26T06:22:34.870092Z","shell.execute_reply":"2023-08-26T06:22:34.890426Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Window based multi-head self-attention\n\nUsually Transformers perform global self-attention, where the relationships between\na token and all other tokens are computed. The global computation leads to quadratic\ncomplexity with respect to the number of tokens. Here, as the [original paper](https://arxiv.org/abs/2103.14030)\nsuggests, we compute self-attention within local windows, in a non-overlapping manner.\nGlobal self-attention leads to quadratic computational complexity in the number of patches,\nwhereas window-based self-attention leads to linear complexity and is easily scalable.","metadata":{}},{"cell_type":"code","source":"\nclass WindowAttention(layers.Layer):\n    def __init__(\n        self, dim, window_size, num_heads, qkv_bias=True, dropout_rate=0.0, **kwargs\n    ):\n        super(WindowAttention, self).__init__(**kwargs)\n        self.dim = dim\n        self.window_size = window_size\n        self.num_heads = num_heads\n        self.scale = (dim // num_heads) ** -0.5\n        self.qkv = layers.Dense(dim * 3, use_bias=qkv_bias)\n        self.dropout = layers.Dropout(dropout_rate)\n        self.proj = layers.Dense(dim)\n\n    def build(self, input_shape):\n        num_window_elements = (2 * self.window_size[0] - 1) * (\n            2 * self.window_size[1] - 1\n        )\n        self.relative_position_bias_table = self.add_weight(\n            shape=(num_window_elements, self.num_heads),\n            initializer=tf.initializers.Zeros(),\n            trainable=True,\n        )\n        coords_h = np.arange(self.window_size[0])\n        coords_w = np.arange(self.window_size[1])\n        coords_matrix = np.meshgrid(coords_h, coords_w, indexing=\"ij\")\n        coords = np.stack(coords_matrix)\n        coords_flatten = coords.reshape(2, -1)\n        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n        relative_coords = relative_coords.transpose([1, 2, 0])\n        relative_coords[:, :, 0] += self.window_size[0] - 1\n        relative_coords[:, :, 1] += self.window_size[1] - 1\n        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n        relative_position_index = relative_coords.sum(-1)\n\n        self.relative_position_index = tf.Variable(\n            initial_value=tf.convert_to_tensor(relative_position_index), trainable=False\n        )\n\n    def call(self, x, mask=None):\n        _, size, channels = x.shape\n        head_dim = channels // self.num_heads\n        x_qkv = self.qkv(x)\n        x_qkv = tf.reshape(x_qkv, shape=(-1, size, 3, self.num_heads, head_dim))\n        x_qkv = tf.transpose(x_qkv, perm=(2, 0, 3, 1, 4))\n        q, k, v = x_qkv[0], x_qkv[1], x_qkv[2]\n        q = q * self.scale\n        k = tf.transpose(k, perm=(0, 1, 3, 2))\n        attn = q @ k\n\n        num_window_elements = self.window_size[0] * self.window_size[1]\n        relative_position_index_flat = tf.reshape(\n            self.relative_position_index, shape=(-1,)\n        )\n        relative_position_bias = tf.gather(\n            self.relative_position_bias_table, relative_position_index_flat\n        )\n        relative_position_bias = tf.reshape(\n            relative_position_bias, shape=(num_window_elements, num_window_elements, -1)\n        )\n        relative_position_bias = tf.transpose(relative_position_bias, perm=(2, 0, 1))\n        attn = attn + tf.expand_dims(relative_position_bias, axis=0)\n\n        if mask is not None:\n            nW = mask.get_shape()[0]\n            mask_float = tf.cast(\n                tf.expand_dims(tf.expand_dims(mask, axis=1), axis=0), tf.float32\n            )\n            attn = (\n                tf.reshape(attn, shape=(-1, nW, self.num_heads, size, size))\n                + mask_float\n            )\n            attn = tf.reshape(attn, shape=(-1, self.num_heads, size, size))\n            attn = keras.activations.softmax(attn, axis=-1)\n        else:\n            attn = keras.activations.softmax(attn, axis=-1)\n        attn = self.dropout(attn)\n\n        x_qkv = attn @ v\n        x_qkv = tf.transpose(x_qkv, perm=(0, 2, 1, 3))\n        x_qkv = tf.reshape(x_qkv, shape=(-1, size, channels))\n        x_qkv = self.proj(x_qkv)\n        x_qkv = self.dropout(x_qkv)\n        return x_qkv\n","metadata":{"execution":{"iopub.status.busy":"2023-08-26T06:22:41.260761Z","iopub.execute_input":"2023-08-26T06:22:41.261229Z","iopub.status.idle":"2023-08-26T06:22:41.299780Z","shell.execute_reply.started":"2023-08-26T06:22:41.261184Z","shell.execute_reply":"2023-08-26T06:22:41.298992Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## The complete Swin Transformer model\n\nFinally, we put together the complete Swin Transformer by replacing the standard multi-head\nattention (MHA) with shifted windows attention. As suggested in the\noriginal paper, we create a model comprising of a shifted window-based MHA\nlayer, followed by a 2-layer MLP with GELU nonlinearity in between, applying\n`LayerNormalization` before each MSA layer and each MLP, and a residual\nconnection after each of these layers.\n\nNotice that we only create a simple MLP with 2 Dense and\n2 Dropout layers. Often you will see models using ResNet-50 as the MLP which is\nquite standard in the literature. However in this paper the authors use a\n2-layer MLP with GELU nonlinearity in between.","metadata":{}},{"cell_type":"code","source":"\nclass SwinTransformer(layers.Layer):\n    def __init__(\n        self,\n        dim,\n        num_patch,\n        num_heads,\n        window_size=7,\n        shift_size=0,\n        num_mlp=1024,\n        qkv_bias=True,\n        dropout_rate=0.0,\n        **kwargs,\n    ):\n        super(SwinTransformer, self).__init__(**kwargs)\n\n        self.dim = dim  # number of input dimensions\n        self.num_patch = num_patch  # number of embedded patches\n        self.num_heads = num_heads  # number of attention heads\n        self.window_size = window_size  # size of window\n        self.shift_size = shift_size  # size of window shift\n        self.num_mlp = num_mlp  # number of MLP nodes\n\n        self.norm1 = layers.LayerNormalization(epsilon=1e-5)\n        self.attn = WindowAttention(\n            dim,\n            window_size=(self.window_size, self.window_size),\n            num_heads=num_heads,\n            qkv_bias=qkv_bias,\n            dropout_rate=dropout_rate,\n        )\n        self.drop_path = DropPath(dropout_rate)\n        self.norm2 = layers.LayerNormalization(epsilon=1e-5)\n\n        self.mlp = keras.Sequential(\n            [\n                layers.Dense(num_mlp),\n                layers.Activation(keras.activations.gelu),\n                layers.Dropout(dropout_rate),\n                layers.Dense(dim),\n                layers.Dropout(dropout_rate),\n            ]\n        )\n\n        if min(self.num_patch) < self.window_size:\n            self.shift_size = 0\n            self.window_size = min(self.num_patch)\n\n    def build(self, input_shape):\n        if self.shift_size == 0:\n            self.attn_mask = None\n        else:\n            height, width = self.num_patch\n            h_slices = (\n                slice(0, -self.window_size),\n                slice(-self.window_size, -self.shift_size),\n                slice(-self.shift_size, None),\n            )\n            w_slices = (\n                slice(0, -self.window_size),\n                slice(-self.window_size, -self.shift_size),\n                slice(-self.shift_size, None),\n            )\n            mask_array = np.zeros((1, height, width, 1))\n            count = 0\n            for h in h_slices:\n                for w in w_slices:\n                    mask_array[:, h, w, :] = count\n                    count += 1\n            mask_array = tf.convert_to_tensor(mask_array)\n\n            # mask array to windows\n            mask_windows = window_partition(mask_array, self.window_size)\n            mask_windows = tf.reshape(\n                mask_windows, shape=[-1, self.window_size * self.window_size]\n            )\n            attn_mask = tf.expand_dims(mask_windows, axis=1) - tf.expand_dims(\n                mask_windows, axis=2\n            )\n            attn_mask = tf.where(attn_mask != 0, -100.0, attn_mask)\n            attn_mask = tf.where(attn_mask == 0, 0.0, attn_mask)\n            self.attn_mask = tf.Variable(initial_value=attn_mask, trainable=False)\n\n    def call(self, x):\n        height, width = self.num_patch\n        _, num_patches_before, channels = x.shape\n        x_skip = x\n        x = self.norm1(x)\n        x = tf.reshape(x, shape=(-1, height, width, channels))\n        if self.shift_size > 0:\n            shifted_x = tf.roll(\n                x, shift=[-self.shift_size, -self.shift_size], axis=[1, 2]\n            )\n        else:\n            shifted_x = x\n\n        x_windows = window_partition(shifted_x, self.window_size)\n        x_windows = tf.reshape(\n            x_windows, shape=(-1, self.window_size * self.window_size, channels)\n        )\n        attn_windows = self.attn(x_windows, mask=self.attn_mask)\n\n        attn_windows = tf.reshape(\n            attn_windows, shape=(-1, self.window_size, self.window_size, channels)\n        )\n        shifted_x = window_reverse(\n            attn_windows, self.window_size, height, width, channels\n        )\n        if self.shift_size > 0:\n            x = tf.roll(\n                shifted_x, shift=[self.shift_size, self.shift_size], axis=[1, 2]\n            )\n        else:\n            x = shifted_x\n\n        x = tf.reshape(x, shape=(-1, height * width, channels))\n        x = self.drop_path(x)\n        x = x_skip + x\n        x_skip = x\n        x = self.norm2(x)\n        x = self.mlp(x)\n        x = self.drop_path(x)\n        x = x_skip + x\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2023-08-26T06:22:47.851066Z","iopub.execute_input":"2023-08-26T06:22:47.851447Z","iopub.status.idle":"2023-08-26T06:22:47.891235Z","shell.execute_reply.started":"2023-08-26T06:22:47.851401Z","shell.execute_reply":"2023-08-26T06:22:47.890385Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Model training and evaluation\n\n### Extract and embed patches\n\nWe first create 3 layers to help us extract, embed and merge patches from the\nimages on top of which we will later use the Swin Transformer class we built.","metadata":{}},{"cell_type":"code","source":"\nclass PatchExtract(layers.Layer):\n    def __init__(self, patch_size, **kwargs):\n        super(PatchExtract, self).__init__(**kwargs)\n        self.patch_size_x = patch_size[0]\n        self.patch_size_y = patch_size[0]\n\n    def call(self, images):\n        batch_size = tf.shape(images)[0]\n        patches = tf.image.extract_patches(\n            images=images,\n            sizes=(1, self.patch_size_x, self.patch_size_y, 1),\n            strides=(1, self.patch_size_x, self.patch_size_y, 1),\n            rates=(1, 1, 1, 1),\n            padding=\"VALID\",\n        )\n        patch_dim = patches.shape[-1]\n        patch_num = patches.shape[1]\n        return tf.reshape(patches, (batch_size, patch_num * patch_num, patch_dim))\n\n\nclass PatchEmbedding(layers.Layer):\n    def __init__(self, num_patch, embed_dim, **kwargs):\n        super(PatchEmbedding, self).__init__(**kwargs)\n        self.num_patch = num_patch\n        self.proj = layers.Dense(embed_dim)\n        self.pos_embed = layers.Embedding(input_dim=num_patch, output_dim=embed_dim)\n\n    def call(self, patch):\n        pos = tf.range(start=0, limit=self.num_patch, delta=1)\n        return self.proj(patch) + self.pos_embed(pos)\n\n\nclass PatchMerging(tf.keras.layers.Layer):\n    def __init__(self, num_patch, embed_dim):\n        super(PatchMerging, self).__init__()\n        self.num_patch = num_patch\n        self.embed_dim = embed_dim\n        self.linear_trans = layers.Dense(2 * embed_dim, use_bias=False)\n\n    def call(self, x):\n        height, width = self.num_patch\n        _, _, C = x.get_shape().as_list()\n        x = tf.reshape(x, shape=(-1, height, width, C))\n        x0 = x[:, 0::2, 0::2, :]\n        x1 = x[:, 1::2, 0::2, :]\n        x2 = x[:, 0::2, 1::2, :]\n        x3 = x[:, 1::2, 1::2, :]\n        x = tf.concat((x0, x1, x2, x3), axis=-1)\n        x = tf.reshape(x, shape=(-1, (height // 2) * (width // 2), 4 * C))\n        return self.linear_trans(x)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-26T06:22:55.345074Z","iopub.execute_input":"2023-08-26T06:22:55.345437Z","iopub.status.idle":"2023-08-26T06:22:55.372424Z","shell.execute_reply.started":"2023-08-26T06:22:55.345394Z","shell.execute_reply":"2023-08-26T06:22:55.371318Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"### Build the model\n\nWe put together the Swin Transformer model.","metadata":{}},{"cell_type":"code","source":"num_classes=4\ninput = layers.Input(input_shape)\nx = layers.RandomCrop(image_dimension, image_dimension)(input)\nx = layers.RandomFlip(\"horizontal\")(x)\nx = PatchExtract(patch_size)(x)\nx = PatchEmbedding(num_patch_x * num_patch_y, embed_dim)(x)\nx = SwinTransformer(\n    dim=embed_dim,\n    num_patch=(num_patch_x, num_patch_y),\n    num_heads=num_heads,\n    window_size=window_size,\n    shift_size=0,\n    num_mlp=num_mlp,\n    qkv_bias=qkv_bias,\n    dropout_rate=dropout_rate,\n)(x)\nx = SwinTransformer(\n    dim=embed_dim,\n    num_patch=(num_patch_x, num_patch_y),\n    num_heads=num_heads,\n    window_size=window_size,\n    shift_size=shift_size,\n    num_mlp=num_mlp,\n    qkv_bias=qkv_bias,\n    dropout_rate=dropout_rate,\n)(x)\nx = PatchMerging((num_patch_x, num_patch_y), embed_dim=embed_dim)(x)\nx = layers.GlobalAveragePooling1D()(x)\noutput = layers.Dense(num_classes, activation=\"softmax\")(x)","metadata":{"execution":{"iopub.status.busy":"2023-08-26T08:10:51.221183Z","iopub.execute_input":"2023-08-26T08:10:51.221794Z","iopub.status.idle":"2023-08-26T08:10:51.695179Z","shell.execute_reply.started":"2023-08-26T08:10:51.221754Z","shell.execute_reply":"2023-08-26T08:10:51.694234Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"### Train on CIFAR-100\n\nWe train the model on CIFAR-100. Here, we only train the model\nfor 40 epochs to keep the training time short in this example.\nIn practice, you should train for 150 epochs to reach convergence.","metadata":{}},{"cell_type":"code","source":"image_gen = ImageDataGenerator(preprocessing_function= tf.keras.applications.mobilenet_v2.preprocess_input)\ntrain = image_gen.flow_from_dataframe(dataframe= train_set,x_col=\"filepaths\",y_col=\"labels\",\n                                      target_size=(128,128),\n                                      color_mode='rgb',\n                                      class_mode=\"categorical\", #used for Sequential Model\n                                      batch_size=32,\n                                      shuffle=False            #do not shuffle data\n                                     )\ntest = image_gen.flow_from_dataframe(dataframe= test_images,x_col=\"filepaths\", y_col=\"labels\",\n                                     target_size=(128,128),\n                                     color_mode='rgb',\n                                     class_mode=\"categorical\",\n                                     batch_size=32,\n                                     shuffle= False\n                                    )\nval = image_gen.flow_from_dataframe(dataframe= val_set,x_col=\"filepaths\", y_col=\"labels\",\n                                    target_size=(128,128),\n                                    color_mode= 'rgb',\n                                    class_mode=\"categorical\",\n                                    batch_size=32,\n                                    shuffle=False\n                                   )","metadata":{"execution":{"iopub.status.busy":"2023-08-26T07:37:16.413533Z","iopub.execute_input":"2023-08-26T07:37:16.413833Z","iopub.status.idle":"2023-08-26T07:37:19.404389Z","shell.execute_reply.started":"2023-08-26T07:37:16.413792Z","shell.execute_reply":"2023-08-26T07:37:19.403611Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"Found 4569 validated image filenames belonging to 4 classes.\nFound 857 validated image filenames belonging to 4 classes.\nFound 1143 validated image filenames belonging to 4 classes.\n","output_type":"stream"}]},{"cell_type":"code","source":"model = keras.Model(input, output)\nmodel.compile(\n    loss=keras.losses.CategoricalCrossentropy(label_smoothing=label_smoothing),\n    #optimizer=tfa.optimizers.AdamW(\n    #    learning_rate=learning_rate, weight_decay=weight_decay\n    #),\n    optimizer=keras.optimizers.Adam(learning_rate=1e-3, decay=weight_decay),\n    metrics=[\n        keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n        keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n    ],\n)\n\nhistory = model.fit(\n    train,\n    #y_train,\n    #batch_size=128,\n    epochs=num_epochs,\n    #validation_split=validation_split,\n    #validation_data=(x_test,y_test),\n    validation_data=val,\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-26T08:10:59.781961Z","iopub.execute_input":"2023-08-26T08:10:59.782246Z","iopub.status.idle":"2023-08-26T08:24:05.485561Z","shell.execute_reply.started":"2023-08-26T08:10:59.782212Z","shell.execute_reply":"2023-08-26T08:24:05.484704Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"Epoch 1/40\n143/143 [==============================] - 23s 139ms/step - loss: 1.7990 - accuracy: 0.5303 - top-5-accuracy: 1.0000 - val_loss: 0.9461 - val_accuracy: 0.6964 - val_top-5-accuracy: 1.0000\nEpoch 2/40\n143/143 [==============================] - 18s 127ms/step - loss: 0.9113 - accuracy: 0.6982 - top-5-accuracy: 1.0000 - val_loss: 0.8647 - val_accuracy: 0.7332 - val_top-5-accuracy: 1.0000\nEpoch 3/40\n143/143 [==============================] - 19s 130ms/step - loss: 0.8584 - accuracy: 0.7339 - top-5-accuracy: 1.0000 - val_loss: 0.7997 - val_accuracy: 0.7647 - val_top-5-accuracy: 1.0000\nEpoch 4/40\n143/143 [==============================] - 19s 130ms/step - loss: 0.7590 - accuracy: 0.7897 - top-5-accuracy: 1.0000 - val_loss: 0.7245 - val_accuracy: 0.8206 - val_top-5-accuracy: 1.0000\nEpoch 5/40\n143/143 [==============================] - 18s 128ms/step - loss: 0.7127 - accuracy: 0.8183 - top-5-accuracy: 1.0000 - val_loss: 0.7623 - val_accuracy: 0.7857 - val_top-5-accuracy: 1.0000\nEpoch 6/40\n143/143 [==============================] - 18s 128ms/step - loss: 0.6822 - accuracy: 0.8378 - top-5-accuracy: 1.0000 - val_loss: 0.6834 - val_accuracy: 0.8259 - val_top-5-accuracy: 1.0000\nEpoch 7/40\n143/143 [==============================] - 19s 130ms/step - loss: 0.6285 - accuracy: 0.8693 - top-5-accuracy: 1.0000 - val_loss: 0.6560 - val_accuracy: 0.8408 - val_top-5-accuracy: 1.0000\nEpoch 8/40\n143/143 [==============================] - 19s 131ms/step - loss: 0.6022 - accuracy: 0.8860 - top-5-accuracy: 1.0000 - val_loss: 0.6602 - val_accuracy: 0.8609 - val_top-5-accuracy: 1.0000\nEpoch 9/40\n143/143 [==============================] - 18s 128ms/step - loss: 0.5889 - accuracy: 0.8963 - top-5-accuracy: 1.0000 - val_loss: 0.6658 - val_accuracy: 0.8635 - val_top-5-accuracy: 1.0000\nEpoch 10/40\n143/143 [==============================] - 19s 132ms/step - loss: 0.5757 - accuracy: 0.9103 - top-5-accuracy: 1.0000 - val_loss: 0.6237 - val_accuracy: 0.8836 - val_top-5-accuracy: 1.0000\nEpoch 11/40\n143/143 [==============================] - 19s 131ms/step - loss: 0.5412 - accuracy: 0.9227 - top-5-accuracy: 1.0000 - val_loss: 0.6525 - val_accuracy: 0.8513 - val_top-5-accuracy: 1.0000\nEpoch 12/40\n143/143 [==============================] - 18s 128ms/step - loss: 0.5180 - accuracy: 0.9361 - top-5-accuracy: 1.0000 - val_loss: 0.5857 - val_accuracy: 0.8880 - val_top-5-accuracy: 1.0000\nEpoch 13/40\n143/143 [==============================] - 20s 140ms/step - loss: 0.5232 - accuracy: 0.9352 - top-5-accuracy: 1.0000 - val_loss: 0.6131 - val_accuracy: 0.8801 - val_top-5-accuracy: 1.0000\nEpoch 14/40\n143/143 [==============================] - 21s 145ms/step - loss: 0.5028 - accuracy: 0.9448 - top-5-accuracy: 1.0000 - val_loss: 0.5986 - val_accuracy: 0.8880 - val_top-5-accuracy: 1.0000\nEpoch 15/40\n143/143 [==============================] - 21s 150ms/step - loss: 0.5005 - accuracy: 0.9477 - top-5-accuracy: 1.0000 - val_loss: 0.6036 - val_accuracy: 0.8889 - val_top-5-accuracy: 1.0000\nEpoch 16/40\n143/143 [==============================] - 21s 144ms/step - loss: 0.4874 - accuracy: 0.9584 - top-5-accuracy: 1.0000 - val_loss: 0.5868 - val_accuracy: 0.9003 - val_top-5-accuracy: 1.0000\nEpoch 17/40\n143/143 [==============================] - 21s 147ms/step - loss: 0.4774 - accuracy: 0.9637 - top-5-accuracy: 1.0000 - val_loss: 0.5682 - val_accuracy: 0.9064 - val_top-5-accuracy: 1.0000\nEpoch 18/40\n143/143 [==============================] - 20s 140ms/step - loss: 0.4623 - accuracy: 0.9724 - top-5-accuracy: 1.0000 - val_loss: 0.5592 - val_accuracy: 0.9099 - val_top-5-accuracy: 1.0000\nEpoch 19/40\n143/143 [==============================] - 18s 129ms/step - loss: 0.4598 - accuracy: 0.9718 - top-5-accuracy: 1.0000 - val_loss: 0.6380 - val_accuracy: 0.8810 - val_top-5-accuracy: 1.0000\nEpoch 20/40\n143/143 [==============================] - 19s 130ms/step - loss: 0.4545 - accuracy: 0.9744 - top-5-accuracy: 1.0000 - val_loss: 0.5483 - val_accuracy: 0.9213 - val_top-5-accuracy: 1.0000\nEpoch 21/40\n143/143 [==============================] - 19s 130ms/step - loss: 0.4492 - accuracy: 0.9779 - top-5-accuracy: 1.0000 - val_loss: 0.5788 - val_accuracy: 0.9160 - val_top-5-accuracy: 1.0000\nEpoch 22/40\n143/143 [==============================] - 18s 128ms/step - loss: 0.4383 - accuracy: 0.9849 - top-5-accuracy: 1.0000 - val_loss: 0.5352 - val_accuracy: 0.9160 - val_top-5-accuracy: 1.0000\nEpoch 23/40\n143/143 [==============================] - 18s 129ms/step - loss: 0.4371 - accuracy: 0.9825 - top-5-accuracy: 1.0000 - val_loss: 0.5575 - val_accuracy: 0.9116 - val_top-5-accuracy: 1.0000\nEpoch 24/40\n143/143 [==============================] - 18s 128ms/step - loss: 0.4345 - accuracy: 0.9812 - top-5-accuracy: 1.0000 - val_loss: 0.5568 - val_accuracy: 0.9160 - val_top-5-accuracy: 1.0000\nEpoch 25/40\n143/143 [==============================] - 19s 133ms/step - loss: 0.4324 - accuracy: 0.9838 - top-5-accuracy: 1.0000 - val_loss: 0.5435 - val_accuracy: 0.9143 - val_top-5-accuracy: 1.0000\nEpoch 26/40\n143/143 [==============================] - 18s 125ms/step - loss: 0.4373 - accuracy: 0.9829 - top-5-accuracy: 1.0000 - val_loss: 0.5400 - val_accuracy: 0.9221 - val_top-5-accuracy: 1.0000\nEpoch 27/40\n143/143 [==============================] - 19s 130ms/step - loss: 0.4238 - accuracy: 0.9880 - top-5-accuracy: 1.0000 - val_loss: 0.5213 - val_accuracy: 0.9291 - val_top-5-accuracy: 1.0000\nEpoch 28/40\n143/143 [==============================] - 19s 133ms/step - loss: 0.4269 - accuracy: 0.9862 - top-5-accuracy: 1.0000 - val_loss: 0.5627 - val_accuracy: 0.9186 - val_top-5-accuracy: 1.0000\nEpoch 29/40\n143/143 [==============================] - 18s 126ms/step - loss: 0.4197 - accuracy: 0.9886 - top-5-accuracy: 1.0000 - val_loss: 0.5614 - val_accuracy: 0.9125 - val_top-5-accuracy: 1.0000\nEpoch 30/40\n143/143 [==============================] - 19s 130ms/step - loss: 0.4253 - accuracy: 0.9866 - top-5-accuracy: 1.0000 - val_loss: 0.5570 - val_accuracy: 0.9055 - val_top-5-accuracy: 1.0000\nEpoch 31/40\n143/143 [==============================] - 19s 131ms/step - loss: 0.4119 - accuracy: 0.9912 - top-5-accuracy: 1.0000 - val_loss: 0.5276 - val_accuracy: 0.9178 - val_top-5-accuracy: 1.0000\nEpoch 32/40\n143/143 [==============================] - 19s 130ms/step - loss: 0.4120 - accuracy: 0.9915 - top-5-accuracy: 1.0000 - val_loss: 0.5331 - val_accuracy: 0.9204 - val_top-5-accuracy: 1.0000\nEpoch 33/40\n143/143 [==============================] - 18s 129ms/step - loss: 0.4125 - accuracy: 0.9917 - top-5-accuracy: 1.0000 - val_loss: 0.5304 - val_accuracy: 0.9221 - val_top-5-accuracy: 1.0000\nEpoch 34/40\n143/143 [==============================] - 19s 130ms/step - loss: 0.4103 - accuracy: 0.9926 - top-5-accuracy: 1.0000 - val_loss: 0.5168 - val_accuracy: 0.9265 - val_top-5-accuracy: 1.0000\nEpoch 35/40\n143/143 [==============================] - 19s 132ms/step - loss: 0.4168 - accuracy: 0.9893 - top-5-accuracy: 1.0000 - val_loss: 0.5247 - val_accuracy: 0.9291 - val_top-5-accuracy: 1.0000\nEpoch 36/40\n143/143 [==============================] - 18s 125ms/step - loss: 0.4128 - accuracy: 0.9923 - top-5-accuracy: 1.0000 - val_loss: 0.5104 - val_accuracy: 0.9318 - val_top-5-accuracy: 1.0000\nEpoch 37/40\n143/143 [==============================] - 19s 132ms/step - loss: 0.4067 - accuracy: 0.9932 - top-5-accuracy: 1.0000 - val_loss: 0.5138 - val_accuracy: 0.9335 - val_top-5-accuracy: 1.0000\nEpoch 38/40\n143/143 [==============================] - 18s 127ms/step - loss: 0.4022 - accuracy: 0.9952 - top-5-accuracy: 1.0000 - val_loss: 0.5137 - val_accuracy: 0.9300 - val_top-5-accuracy: 1.0000\nEpoch 39/40\n143/143 [==============================] - 18s 129ms/step - loss: 0.4052 - accuracy: 0.9923 - top-5-accuracy: 1.0000 - val_loss: 0.5389 - val_accuracy: 0.9116 - val_top-5-accuracy: 1.0000\nEpoch 40/40\n143/143 [==============================] - 18s 129ms/step - loss: 0.4046 - accuracy: 0.9934 - top-5-accuracy: 1.0000 - val_loss: 0.5716 - val_accuracy: 0.8941 - val_top-5-accuracy: 1.0000\n","output_type":"stream"}]},{"cell_type":"code","source":"model = keras.Model(input, output)\nmodel.compile(\n    loss=keras.losses.CategoricalCrossentropy(label_smoothing=label_smoothing),\n    optimizer=tfa.optimizers.AdamW(\n        learning_rate=learning_rate, weight_decay=weight_decay\n    ),\n    #optimizer=keras.optimizers.Adam(learning_rate=1e-2, decay=weight_decay),\n    metrics=[\n        keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n        keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n    ],\n)\n\nhistory = model.fit(\n    x_train,\n    y_train,\n    batch_size=256,\n    epochs=num_epochs,\n    #validation_split=validation_split,\n    validation_data=(x_test,y_test),\n    #validation_data=val,\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-26T06:42:24.162184Z","iopub.execute_input":"2023-08-26T06:42:24.162932Z","iopub.status.idle":"2023-08-26T06:43:09.641562Z","shell.execute_reply.started":"2023-08-26T06:42:24.162890Z","shell.execute_reply":"2023-08-26T06:43:09.640721Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"Epoch 1/40\n12/12 [==============================] - 5s 133ms/step - loss: 8.8973 - accuracy: 0.3122 - top-5-accuracy: 1.0000 - val_loss: 17.4221 - val_accuracy: 0.1954 - val_top-5-accuracy: 1.0000\nEpoch 2/40\n12/12 [==============================] - 1s 56ms/step - loss: 5.5362 - accuracy: 0.4028 - top-5-accuracy: 1.0000 - val_loss: 5.0962 - val_accuracy: 0.2487 - val_top-5-accuracy: 1.0000\nEpoch 3/40\n12/12 [==============================] - 1s 56ms/step - loss: 2.4793 - accuracy: 0.4659 - top-5-accuracy: 1.0000 - val_loss: 3.0304 - val_accuracy: 0.3579 - val_top-5-accuracy: 1.0000\nEpoch 4/40\n12/12 [==============================] - 1s 55ms/step - loss: 1.5946 - accuracy: 0.5582 - top-5-accuracy: 1.0000 - val_loss: 3.7525 - val_accuracy: 0.2843 - val_top-5-accuracy: 1.0000\nEpoch 5/40\n12/12 [==============================] - 1s 55ms/step - loss: 1.2578 - accuracy: 0.6010 - top-5-accuracy: 1.0000 - val_loss: 2.1842 - val_accuracy: 0.3985 - val_top-5-accuracy: 1.0000\nEpoch 6/40\n12/12 [==============================] - 1s 54ms/step - loss: 1.0496 - accuracy: 0.6505 - top-5-accuracy: 1.0000 - val_loss: 1.9753 - val_accuracy: 0.3756 - val_top-5-accuracy: 1.0000\nEpoch 7/40\n12/12 [==============================] - 1s 57ms/step - loss: 0.9932 - accuracy: 0.6610 - top-5-accuracy: 1.0000 - val_loss: 1.8133 - val_accuracy: 0.4695 - val_top-5-accuracy: 1.0000\nEpoch 8/40\n12/12 [==============================] - 1s 57ms/step - loss: 0.8991 - accuracy: 0.7052 - top-5-accuracy: 1.0000 - val_loss: 1.6897 - val_accuracy: 0.4467 - val_top-5-accuracy: 1.0000\nEpoch 9/40\n12/12 [==============================] - 1s 54ms/step - loss: 0.8614 - accuracy: 0.7310 - top-5-accuracy: 1.0000 - val_loss: 1.6612 - val_accuracy: 0.4949 - val_top-5-accuracy: 1.0000\nEpoch 10/40\n12/12 [==============================] - 1s 54ms/step - loss: 0.8560 - accuracy: 0.7293 - top-5-accuracy: 1.0000 - val_loss: 1.6636 - val_accuracy: 0.4721 - val_top-5-accuracy: 1.0000\nEpoch 11/40\n12/12 [==============================] - 1s 53ms/step - loss: 0.8431 - accuracy: 0.7328 - top-5-accuracy: 1.0000 - val_loss: 1.7686 - val_accuracy: 0.5076 - val_top-5-accuracy: 1.0000\nEpoch 12/40\n12/12 [==============================] - 1s 55ms/step - loss: 0.8213 - accuracy: 0.7613 - top-5-accuracy: 1.0000 - val_loss: 1.5907 - val_accuracy: 0.5330 - val_top-5-accuracy: 1.0000\nEpoch 13/40\n12/12 [==============================] - 1s 55ms/step - loss: 0.8109 - accuracy: 0.7571 - top-5-accuracy: 1.0000 - val_loss: 1.5327 - val_accuracy: 0.5305 - val_top-5-accuracy: 1.0000\nEpoch 14/40\n12/12 [==============================] - 1s 55ms/step - loss: 0.8311 - accuracy: 0.7467 - top-5-accuracy: 1.0000 - val_loss: 1.7037 - val_accuracy: 0.4416 - val_top-5-accuracy: 1.0000\nEpoch 15/40\n12/12 [==============================] - 1s 69ms/step - loss: 0.9097 - accuracy: 0.7080 - top-5-accuracy: 1.0000 - val_loss: 2.3791 - val_accuracy: 0.4010 - val_top-5-accuracy: 1.0000\nEpoch 16/40\n12/12 [==============================] - 1s 86ms/step - loss: 0.8907 - accuracy: 0.7244 - top-5-accuracy: 1.0000 - val_loss: 1.7529 - val_accuracy: 0.4594 - val_top-5-accuracy: 1.0000\nEpoch 17/40\n12/12 [==============================] - 1s 57ms/step - loss: 0.7990 - accuracy: 0.7620 - top-5-accuracy: 1.0000 - val_loss: 1.6415 - val_accuracy: 0.5152 - val_top-5-accuracy: 1.0000\nEpoch 18/40\n12/12 [==============================] - 1s 54ms/step - loss: 0.7902 - accuracy: 0.7725 - top-5-accuracy: 1.0000 - val_loss: 1.6806 - val_accuracy: 0.5609 - val_top-5-accuracy: 1.0000\nEpoch 19/40\n12/12 [==============================] - 1s 57ms/step - loss: 0.7371 - accuracy: 0.8080 - top-5-accuracy: 1.0000 - val_loss: 1.6457 - val_accuracy: 0.5635 - val_top-5-accuracy: 1.0000\nEpoch 20/40\n12/12 [==============================] - 1s 54ms/step - loss: 0.7341 - accuracy: 0.8115 - top-5-accuracy: 1.0000 - val_loss: 1.5580 - val_accuracy: 0.4898 - val_top-5-accuracy: 1.0000\nEpoch 21/40\n12/12 [==============================] - 1s 53ms/step - loss: 0.7264 - accuracy: 0.8098 - top-5-accuracy: 1.0000 - val_loss: 1.6665 - val_accuracy: 0.5381 - val_top-5-accuracy: 1.0000\nEpoch 22/40\n12/12 [==============================] - 1s 61ms/step - loss: 0.7018 - accuracy: 0.8209 - top-5-accuracy: 1.0000 - val_loss: 1.7306 - val_accuracy: 0.5558 - val_top-5-accuracy: 1.0000\nEpoch 23/40\n12/12 [==============================] - 1s 54ms/step - loss: 0.6938 - accuracy: 0.8352 - top-5-accuracy: 1.0000 - val_loss: 1.6426 - val_accuracy: 0.5635 - val_top-5-accuracy: 1.0000\nEpoch 24/40\n12/12 [==============================] - 1s 53ms/step - loss: 0.6985 - accuracy: 0.8376 - top-5-accuracy: 1.0000 - val_loss: 1.5822 - val_accuracy: 0.5736 - val_top-5-accuracy: 1.0000\nEpoch 25/40\n12/12 [==============================] - 1s 55ms/step - loss: 0.6855 - accuracy: 0.8334 - top-5-accuracy: 1.0000 - val_loss: 1.6885 - val_accuracy: 0.5888 - val_top-5-accuracy: 1.0000\nEpoch 26/40\n12/12 [==============================] - 1s 56ms/step - loss: 0.6985 - accuracy: 0.8317 - top-5-accuracy: 1.0000 - val_loss: 1.5195 - val_accuracy: 0.5102 - val_top-5-accuracy: 1.0000\nEpoch 27/40\n12/12 [==============================] - 1s 54ms/step - loss: 0.6950 - accuracy: 0.8376 - top-5-accuracy: 1.0000 - val_loss: 1.6086 - val_accuracy: 0.5736 - val_top-5-accuracy: 1.0000\nEpoch 28/40\n12/12 [==============================] - 1s 54ms/step - loss: 0.6899 - accuracy: 0.8296 - top-5-accuracy: 1.0000 - val_loss: 1.6654 - val_accuracy: 0.6193 - val_top-5-accuracy: 1.0000\nEpoch 29/40\n12/12 [==============================] - 1s 55ms/step - loss: 0.6976 - accuracy: 0.8286 - top-5-accuracy: 1.0000 - val_loss: 1.8268 - val_accuracy: 0.4975 - val_top-5-accuracy: 1.0000\nEpoch 30/40\n12/12 [==============================] - 1s 54ms/step - loss: 0.7030 - accuracy: 0.8240 - top-5-accuracy: 1.0000 - val_loss: 1.5291 - val_accuracy: 0.5964 - val_top-5-accuracy: 1.0000\nEpoch 31/40\n12/12 [==============================] - 1s 54ms/step - loss: 0.6802 - accuracy: 0.8334 - top-5-accuracy: 1.0000 - val_loss: 1.6160 - val_accuracy: 0.6117 - val_top-5-accuracy: 1.0000\nEpoch 32/40\n12/12 [==============================] - 1s 54ms/step - loss: 0.6419 - accuracy: 0.8624 - top-5-accuracy: 1.0000 - val_loss: 1.5112 - val_accuracy: 0.6218 - val_top-5-accuracy: 1.0000\nEpoch 33/40\n12/12 [==============================] - 1s 55ms/step - loss: 0.6544 - accuracy: 0.8526 - top-5-accuracy: 1.0000 - val_loss: 1.7533 - val_accuracy: 0.5888 - val_top-5-accuracy: 1.0000\nEpoch 34/40\n12/12 [==============================] - 1s 57ms/step - loss: 0.6364 - accuracy: 0.8662 - top-5-accuracy: 1.0000 - val_loss: 1.7655 - val_accuracy: 0.5660 - val_top-5-accuracy: 1.0000\nEpoch 35/40\n12/12 [==============================] - 1s 56ms/step - loss: 0.6411 - accuracy: 0.8620 - top-5-accuracy: 1.0000 - val_loss: 1.7630 - val_accuracy: 0.5533 - val_top-5-accuracy: 1.0000\nEpoch 36/40\n12/12 [==============================] - 1s 56ms/step - loss: 0.6447 - accuracy: 0.8655 - top-5-accuracy: 1.0000 - val_loss: 1.5666 - val_accuracy: 0.5761 - val_top-5-accuracy: 1.0000\nEpoch 37/40\n12/12 [==============================] - 1s 61ms/step - loss: 0.6237 - accuracy: 0.8770 - top-5-accuracy: 1.0000 - val_loss: 1.5148 - val_accuracy: 0.6574 - val_top-5-accuracy: 1.0000\nEpoch 38/40\n12/12 [==============================] - 1s 56ms/step - loss: 0.6018 - accuracy: 0.8923 - top-5-accuracy: 1.0000 - val_loss: 1.6097 - val_accuracy: 0.6371 - val_top-5-accuracy: 1.0000\nEpoch 39/40\n12/12 [==============================] - 1s 54ms/step - loss: 0.6229 - accuracy: 0.8770 - top-5-accuracy: 1.0000 - val_loss: 1.5191 - val_accuracy: 0.5990 - val_top-5-accuracy: 1.0000\nEpoch 40/40\n12/12 [==============================] - 1s 50ms/step - loss: 0.5970 - accuracy: 0.8916 - top-5-accuracy: 1.0000 - val_loss: 1.3899 - val_accuracy: 0.6421 - val_top-5-accuracy: 1.0000\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Let's visualize the training progress of the model.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10, 6))\n\n# Plot the data\nax.plot(history.history[\"accuracy\"], label=\"Training Accuracy\")\nax.plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\")\n\n# Set the plot title and axis labels\n#ax.set_title('ARIMA Performance')\nax.set_xlabel('Epochs')\nax.set_ylabel('Accuracy')\n\n# Remove the grid lines\nax.grid(False)\n\n# Set the legend\nax.legend(loc='lower right')\nax.set_facecolor('white')\nax.xaxis.label.set_color('black')\nax.yaxis.label.set_color('black')\n\n# Set the color of tick labels to black\nax.tick_params(axis='x', colors='black')\nax.tick_params(axis='y', colors='black')\n#fig.set_facecolor('white')\n# Show the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-25T07:07:17.752240Z","iopub.status.idle":"2023-08-25T07:07:17.752934Z","shell.execute_reply.started":"2023-08-25T07:07:17.752666Z","shell.execute_reply":"2023-08-25T07:07:17.752692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10, 6))\n\n# Plot the data\nax.plot(history.history[\"loss\"], label=\"Training Loss\")\nax.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n\n# Set the plot title and axis labels\n#ax.set_title('ARIMA Performance')\nax.set_xlabel('Epochs')\nax.set_ylabel('Loss')\n\n# Remove the grid lines\nax.grid(False)\n\n# Set the legend\nax.legend(loc='upper right')\nax.set_facecolor('white')\nax.xaxis.label.set_color('black')\nax.yaxis.label.set_color('black')\n\n# Set the color of tick labels to black\nax.tick_params(axis='x', colors='black')\nax.tick_params(axis='y', colors='black')\n#fig.set_facecolor('white')\n# Show the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-25T07:07:17.754256Z","iopub.status.idle":"2023-08-25T07:07:17.754952Z","shell.execute_reply.started":"2023-08-25T07:07:17.754685Z","shell.execute_reply":"2023-08-25T07:07:17.754712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's display the final results of the training on CIFAR-100.","metadata":{}},{"cell_type":"code","source":"loss, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n#loss, accuracy, top_5_accuracy = model.evaluate(x_test)\nprint(f\"Test loss: {round(loss, 2)}\")\nprint(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\nprint(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")","metadata":{"execution":{"iopub.status.busy":"2023-08-25T07:07:17.756282Z","iopub.status.idle":"2023-08-25T07:07:17.757008Z","shell.execute_reply.started":"2023-08-25T07:07:17.756728Z","shell.execute_reply":"2023-08-25T07:07:17.756756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = model.predict(test)\n#pred = np.argmax(pred, axis=1) #pick class with highest  probability\n\n#pred","metadata":{"execution":{"iopub.status.busy":"2023-08-25T07:07:17.758330Z","iopub.status.idle":"2023-08-25T07:07:17.759017Z","shell.execute_reply.started":"2023-08-25T07:07:17.758748Z","shell.execute_reply":"2023-08-25T07:07:17.758774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vit_y_pred = [np.argmax(probas) for probas in pred]\n","metadata":{"execution":{"iopub.status.busy":"2023-08-25T07:07:17.760406Z","iopub.status.idle":"2023-08-25T07:07:17.761107Z","shell.execute_reply.started":"2023-08-25T07:07:17.760853Z","shell.execute_reply":"2023-08-25T07:07:17.760880Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = model.predict(test)\npred = np.argmax(pred, axis=1) #pick class with highest  probability\nlabels = (train.class_indices)\nlabels = dict((v,k) for k,v in labels.items())\npred2 = [labels[k] for k in pred]\nfrom sklearn.metrics import classification_report,accuracy_score","metadata":{"execution":{"iopub.status.busy":"2023-08-25T07:07:17.762400Z","iopub.status.idle":"2023-08-25T07:07:17.763153Z","shell.execute_reply.started":"2023-08-25T07:07:17.762867Z","shell.execute_reply":"2023-08-25T07:07:17.762893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = model.predict(test)\npred = np.argmax(pred, axis=1) #pick class with highest  probability\n\nlabels = (train.class_indices)\nlabels = dict((v,k) for k,v in labels.items())\npred2 = [labels[k] for k in pred]","metadata":{"execution":{"iopub.status.busy":"2023-08-25T07:07:17.764557Z","iopub.status.idle":"2023-08-25T07:07:17.765234Z","shell.execute_reply.started":"2023-08-25T07:07:17.764974Z","shell.execute_reply":"2023-08-25T07:07:17.765001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report,accuracy_score\n\ny_test = test_images.labels # set y_test to the expected output\nprint(classification_report(y_test, pred2))\nprint(\"Accuracy of the Model:\",accuracy_score(y_test, pred2)*100,\"%\")","metadata":{"execution":{"iopub.status.busy":"2023-08-25T07:07:17.766658Z","iopub.status.idle":"2023-08-25T07:07:17.767418Z","shell.execute_reply.started":"2023-08-25T07:07:17.767116Z","shell.execute_reply":"2023-08-25T07:07:17.767144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classification_report.strip()","metadata":{"execution":{"iopub.status.busy":"2023-08-25T07:07:17.768899Z","iopub.status.idle":"2023-08-25T07:07:17.769643Z","shell.execute_reply.started":"2023-08-25T07:07:17.769356Z","shell.execute_reply":"2023-08-25T07:07:17.769383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from sklearn.metrics import classification_report\nreport_lines = classification_report.strip().split('\\n')\nclass_names = []\nprecisions = []\nrecalls = []\nsupports = []\n\nfor line in report_lines[2:-5]:  # Exclude headers and footer lines\n    parts = line.split()\n    class_names.append(parts[0])\n    precisions.append(float(parts[1]))\n    recalls.append(float(parts[2]))\n    supports.append(int(parts[-1]))\n\n# Calculate TP, FP, TN, FN for each class\nTP = [int(recalls[i] * supports[i]) for i in range(len(class_names))]\nFN = [supports[i] - TP[i] for i in range(len(class_names))]\nFP = [int((TP[i] / precisions[i]) - TP[i]) for i in range(len(class_names))]\nTN = [sum(supports) - (TP[i] + FP[i] + FN[i]) for i in range(len(class_names))]\n\n# Print the results\nfor i in range(len(class_names)):\n    print(f'Class: {class_names[i]} - TP: {TP[i]}, FP: {FP[i]}, TN: {TN[i]}, FN: {FN[i]}')","metadata":{"execution":{"iopub.status.busy":"2023-08-25T07:07:17.771000Z","iopub.status.idle":"2023-08-25T07:07:17.771678Z","shell.execute_reply.started":"2023-08-25T07:07:17.771408Z","shell.execute_reply":"2023-08-25T07:07:17.771434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, multilabel_confusion_matrix\n\n# Assuming your model is called 'model' and you have the test data 'x_test' and 'y_test'\npredictions = model.predict(test)\npredicted_labels = (predictions > 0.5).astype(int)  # Thresholding predicted probabilities\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test, predicted_labels)\ny_test = test_images.labels \n# Calculate multilabel confusion matrix\nconfusion_matrix = multilabel_confusion_matrix(y_test, predicted_labels)\nprint(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\nprint(\"Multilabel Confusion Matrix:\")\nprint(confusion_matrix)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-08-25T07:07:17.773105Z","iopub.status.idle":"2023-08-25T07:07:17.773845Z","shell.execute_reply.started":"2023-08-25T07:07:17.773563Z","shell.execute_reply":"2023-08-25T07:07:17.773590Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\n# Assuming your model is called 'model' and you have the test data 'x_test' and 'y_test'\npredictions = model.predict(test)\npredicted_labels = (predictions > 0.5).astype(int)  # Thresholding predicted probabilities\n\n# Reshape the predicted_labels to match the shape of y_test\npredicted_labels = predicted_labels.reshape(y_test.shape)\n\n# Generate classification report\nclassification_report = classification_report(y_test, predicted_labels)\nprint(classification_report)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-25T07:07:17.775299Z","iopub.status.idle":"2023-08-25T07:07:17.775981Z","shell.execute_reply.started":"2023-08-25T07:07:17.775715Z","shell.execute_reply":"2023-08-25T07:07:17.775741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import multilabel_confusion_matrix\n\n# Assuming your model is called 'model' and you have the test data 'x_test' and 'y_test'\npredictions = model.predict(x_test)\npredicted_labels = (predictions > 0.5).astype(int)  # Thresholding predicted probabilities\n\n# Reshape the predicted_labels to match the shape of y_test\npredicted_labels = predicted_labels.reshape(y_test.shape)\n\n# Calculate multilabel confusion matrix\nconfusion_matrix = multilabel_confusion_matrix(y_test, predicted_labels)\nprint(\"Multilabel Confusion Matrix:\")\nfor i, matrix in enumerate(confusion_matrix):\n    print(f\"Class {i + 1}:\")\n    print(matrix)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-25T07:07:17.777378Z","iopub.status.idle":"2023-08-25T07:07:17.778046Z","shell.execute_reply.started":"2023-08-25T07:07:17.777798Z","shell.execute_reply":"2023-08-25T07:07:17.777831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import multilabel_confusion_matrix\n\n# Assuming your model is called 'model' and you have the test data 'x_test' and 'y_test'\npredictions = model.predict(x_test)\npredicted_labels = (predictions > 0.5).astype(int)  # Thresholding predicted probabilities\n\n# Reshape the predicted_labels to match the shape of y_test\npredicted_labels = predicted_labels.reshape(y_test.shape)\n\n# Calculate multilabel confusion matrix\nconfusion_matrix = multilabel_confusion_matrix(y_test, predicted_labels)\n\n# Aggregate the individual confusion matrices into a single 4x4 confusion matrix\noverall_confusion_matrix = np.vstack((\n    np.hstack((confusion_matrix[0], confusion_matrix[1])),\n    np.hstack((confusion_matrix[2], confusion_matrix[3]))\n))\n\nprint(\"Overall Confusion Matrix:\")\nprint(overall_confusion_matrix)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-25T07:07:17.779390Z","iopub.status.idle":"2023-08-25T07:07:17.780105Z","shell.execute_reply.started":"2023-08-25T07:07:17.779851Z","shell.execute_reply":"2023-08-25T07:07:17.779879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix\n\n# Assuming your model is called 'model' and you have the test data 'x_test' and 'y_test'\npredictions = model.predict(x_test)\npredicted_labels = np.argmax(predictions, axis=1)  # Get the predicted labels\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test, predicted_labels)\n\n# Calculate confusion matrix\nconfusion_matrix = confusion_matrix(y_test, predicted_labels)\nprint(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-25T07:07:17.781497Z","iopub.status.idle":"2023-08-25T07:07:17.782202Z","shell.execute_reply.started":"2023-08-25T07:07:17.781947Z","shell.execute_reply":"2023-08-25T07:07:17.781974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#labels = (x_train.class_indices)\nlabels = dict((v,k) for k,v in class_names_label.items())\npred2 = [labels[k] for k in pred]","metadata":{"execution":{"iopub.status.busy":"2023-08-25T07:07:17.783637Z","iopub.status.idle":"2023-08-25T07:07:17.784327Z","shell.execute_reply.started":"2023-08-25T07:07:17.784072Z","shell.execute_reply":"2023-08-25T07:07:17.784098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test.labels","metadata":{"execution":{"iopub.status.busy":"2023-08-25T07:07:17.785638Z","iopub.status.idle":"2023-08-25T07:07:17.786343Z","shell.execute_reply.started":"2023-08-25T07:07:17.786050Z","shell.execute_reply":"2023-08-25T07:07:17.786086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report,accuracy_score\n\n#y_test = test_images.labels # set y_test to the expected output\nprint(classification_report(y_test, vit_y_pred))\nprint(\"Accuracy of the Model:\",accuracy_score(y_test, pred2)*100,\"%\")","metadata":{"execution":{"iopub.status.busy":"2023-08-25T07:07:17.787821Z","iopub.status.idle":"2023-08-25T07:07:17.788551Z","shell.execute_reply.started":"2023-08-25T07:07:17.788249Z","shell.execute_reply":"2023-08-25T07:07:17.788277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnf_matrix = confusion_matrix(y_test, vit_y_pred)","metadata":{"execution":{"iopub.status.busy":"2023-08-25T07:07:17.789858Z","iopub.status.idle":"2023-08-25T07:07:17.790541Z","shell.execute_reply.started":"2023-08-25T07:07:17.790261Z","shell.execute_reply":"2023-08-25T07:07:17.790287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Swin Transformer model we just trained has just 152K parameters, and it gets\nus to ~75% test top-5 accuracy within just 40 epochs without any signs of overfitting\nas well as seen in above graph. This means we can train this network for longer\n(perhaps with a bit more regularization) and obtain even better performance.\nThis performance can further be improved by additional techniques like cosine\ndecay learning rate schedule, other data augmentation techniques. While experimenting,\nI tried training the model for 150 epochs with a slightly higher dropout and greater\nembedding dimensions which pushes the performance to ~72% test accuracy on CIFAR-100\nas you can see in the screenshot.\n\n![Results of training for longer](https://i.imgur.com/9vnQesZ.png)\n\nThe authors present a top-1 accuracy of 87.3% on ImageNet. The authors also present\na number of experiments to study how input sizes, optimizers etc. affect the final\nperformance of this model. The authors further present using this model for object detection,\nsemantic segmentation and instance segmentation as well and report competitive results\nfor these. You are strongly advised to also check out the\n[original paper](https://arxiv.org/abs/2103.14030).\n\nThis example takes inspiration from the official\n[PyTorch](https://github.com/microsoft/Swin-Transformer) and\n[TensorFlow](https://github.com/VcampSoldiers/Swin-Transformer-Tensorflow) implementations.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\ninput_shape = (128, 128, 3)  # Specify the desired input shape\n\n# Swin Transformer block\nclass SwinTransformerBlock(layers.Layer):\n    def __init__(self, embed_dim, num_heads, mlp_dim, qkv_bias, dropout_rate=0.0):\n        super(SwinTransformerBlock, self).__init__()\n\n        self.mlp_dim = mlp_dim\n        self.qkv_bias = qkv_bias\n\n        self.att = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim, dropout=dropout_rate\n        )\n        self.mlp = keras.Sequential(\n            [\n                layers.Dense(units=mlp_dim, activation=keras.activations.gelu),\n                layers.Dropout(rate=dropout_rate),\n                layers.Dense(units=embed_dim),\n                layers.Dropout(rate=dropout_rate),\n            ]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-5)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-5)\n        self.dropout1 = layers.Dropout(rate=dropout_rate)\n        self.dropout2 = layers.Dropout(rate=dropout_rate)\n\n    def call(self, inputs, training=False):\n        # Cross-attention\n        x = inputs\n        x = self.layernorm1(x)\n        attention_output = self.att(x, x, x)\n        attention_output = self.dropout1(attention_output, training=training)\n        out1 = x + attention_output\n\n        # MLP\n        x = out1\n        x = self.layernorm2(x)\n        x = self.mlp(x)\n        x = self.dropout2(x, training=training)\n        out2 = out1 + x\n\n        return out2\n\n# Swin Transformer model\nclass SwinTransformer(keras.Model):\n    def __init__(\n        self,\n        input_shape=input_shape,\n        patch_size=(2, 2),\n        num_heads=4,\n        embed_dim=64,\n        num_mlp_layers=2,\n        mlp_dim=256,\n        qkv_bias=True,\n        dropout_rate=0.0,\n        num_classes=1000,\n    ):\n        super(SwinTransformer, self).__init__()\n\n        self.num_classes = num_classes\n        self.patch_size = patch_size\n        self.embed_dim = embed_dim\n\n        num_patches = (input_shape[0] // patch_size[0]) * (input_shape[1] // patch_size[1])\n        self.patch_proj = layers.Conv2D(embed_dim, patch_size, strides=patch_size, padding=\"valid\")\n        self.pos_emb = self.add_weight(\n            \"pos_emb\", shape=(1, num_patches + 1, embed_dim), initializer=keras.initializers.RandomNormal(), trainable=True\n        )\n        self.dropout = layers.Dropout(rate=dropout_rate)\n        self.blocks = [\n            SwinTransformerBlock(\n                embed_dim=embed_dim,\n                num_heads=num_heads,\n                mlp_dim=mlp_dim,\n                qkv_bias=qkv_bias,\n                dropout_rate=dropout_rate,\n            )\n            for _ in range(num_mlp_layers)\n        ]\n        self.layernorm = layers.LayerNormalization(epsilon=1e-5)\n        self.mlp_head = keras.Sequential(\n            [\n                layers.Dense(units=mlp_dim, activation=keras.activations.gelu),\n                layers.Dropout(rate=dropout_rate),\n                layers.Dense(units=num_classes),\n            ]\n        )\n\n    def call(self, inputs, training=False):\n        # Patch projection\n        x = self.patch_proj(inputs)\n        x = tf.reshape(x, shape=(-1, x.shape[1] * x.shape[2], x.shape[3]))\n\n        # Positional embedding\n        x = x + self.pos_emb\n\n        # Dropout\n        x = self.dropout(x, training=training)\n\n        # Transformer blocks\n        for block in self.blocks:\n            x = block(x, training=training)\n\n        # Layer normalization\n        x = self.layernorm(x)\n\n        # MLP head\n        x = tf.reduce_mean(x, axis=1)\n        x = self.mlp_head(x)\n\n        return x\n\n# Create the Swin Transformer model\nmodel = SwinTransformer()\n\n# Print model summary\nmodel.build(input_shape=(None, *input_shape))\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2023-08-25T07:07:17.791850Z","iopub.status.idle":"2023-08-25T07:07:17.792498Z","shell.execute_reply.started":"2023-08-25T07:07:17.792241Z","shell.execute_reply":"2023-08-25T07:07:17.792267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install timm","metadata":{"execution":{"iopub.status.busy":"2023-08-25T07:10:36.207649Z","iopub.execute_input":"2023-08-25T07:10:36.208006Z","iopub.status.idle":"2023-08-25T07:10:49.401469Z","shell.execute_reply.started":"2023-08-25T07:10:36.207969Z","shell.execute_reply":"2023-08-25T07:10:49.400450Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Collecting timm\n  Downloading timm-0.9.5-py3-none-any.whl (2.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from timm) (0.10.1)\nRequirement already satisfied: torch>=1.7 in /opt/conda/lib/python3.7/site-packages (from timm) (1.9.1)\nCollecting safetensors\n  Downloading safetensors-0.3.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from timm) (6.0)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.7/site-packages (from timm) (0.5.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.7->timm) (4.1.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm) (2.27.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm) (21.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm) (4.63.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm) (3.6.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm) (4.11.3)\nRequirement already satisfied: pillow>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->timm) (9.0.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision->timm) (1.21.5)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.9->huggingface-hub->timm) (3.0.7)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->huggingface-hub->timm) (3.7.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub->timm) (2021.10.8)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub->timm) (2.0.12)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub->timm) (3.3)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub->timm) (1.26.8)\nInstalling collected packages: safetensors, timm\nSuccessfully installed safetensors-0.3.3 timm-0.9.5\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import torchvision\nimport torch\nfrom PIL import Image\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport cv2\nimport glob\nimport math\nimport timm\nfrom PIL import ImageFilter\n#from einops import rearrange\nfrom timm.loss import LabelSmoothingCrossEntropy","metadata":{"execution":{"iopub.status.busy":"2023-08-25T07:11:14.915292Z","iopub.execute_input":"2023-08-25T07:11:14.915834Z","iopub.status.idle":"2023-08-25T07:11:14.925732Z","shell.execute_reply.started":"2023-08-25T07:11:14.915777Z","shell.execute_reply":"2023-08-25T07:11:14.924953Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"timm.list_models('swin*', pretrained=True)","metadata":{"execution":{"iopub.status.busy":"2023-08-25T07:11:43.875352Z","iopub.execute_input":"2023-08-25T07:11:43.876002Z","iopub.status.idle":"2023-08-25T07:11:43.886790Z","shell.execute_reply.started":"2023-08-25T07:11:43.875960Z","shell.execute_reply":"2023-08-25T07:11:43.885865Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"['swin_base_patch4_window7_224.ms_in1k',\n 'swin_base_patch4_window7_224.ms_in22k',\n 'swin_base_patch4_window7_224.ms_in22k_ft_in1k',\n 'swin_base_patch4_window12_384.ms_in1k',\n 'swin_base_patch4_window12_384.ms_in22k',\n 'swin_base_patch4_window12_384.ms_in22k_ft_in1k',\n 'swin_large_patch4_window7_224.ms_in22k',\n 'swin_large_patch4_window7_224.ms_in22k_ft_in1k',\n 'swin_large_patch4_window12_384.ms_in22k',\n 'swin_large_patch4_window12_384.ms_in22k_ft_in1k',\n 'swin_s3_base_224.ms_in1k',\n 'swin_s3_small_224.ms_in1k',\n 'swin_s3_tiny_224.ms_in1k',\n 'swin_small_patch4_window7_224.ms_in1k',\n 'swin_small_patch4_window7_224.ms_in22k',\n 'swin_small_patch4_window7_224.ms_in22k_ft_in1k',\n 'swin_tiny_patch4_window7_224.ms_in1k',\n 'swin_tiny_patch4_window7_224.ms_in22k',\n 'swin_tiny_patch4_window7_224.ms_in22k_ft_in1k',\n 'swinv2_base_window8_256.ms_in1k',\n 'swinv2_base_window12_192.ms_in22k',\n 'swinv2_base_window12to16_192to256.ms_in22k_ft_in1k',\n 'swinv2_base_window12to24_192to384.ms_in22k_ft_in1k',\n 'swinv2_base_window16_256.ms_in1k',\n 'swinv2_cr_small_224.sw_in1k',\n 'swinv2_cr_small_ns_224.sw_in1k',\n 'swinv2_cr_tiny_ns_224.sw_in1k',\n 'swinv2_large_window12_192.ms_in22k',\n 'swinv2_large_window12to16_192to256.ms_in22k_ft_in1k',\n 'swinv2_large_window12to24_192to384.ms_in22k_ft_in1k',\n 'swinv2_small_window8_256.ms_in1k',\n 'swinv2_small_window16_256.ms_in1k',\n 'swinv2_tiny_window8_256.ms_in1k',\n 'swinv2_tiny_window16_256.ms_in1k']"},"metadata":{}}]},{"cell_type":"code","source":"!pip install huggingface_hub","metadata":{"execution":{"iopub.status.busy":"2023-08-25T07:12:36.498954Z","iopub.execute_input":"2023-08-25T07:12:36.499296Z","iopub.status.idle":"2023-08-25T07:12:48.037456Z","shell.execute_reply.started":"2023-08-25T07:12:36.499258Z","shell.execute_reply":"2023-08-25T07:12:48.036143Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.7/site-packages (0.5.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from huggingface_hub) (4.63.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.7/site-packages (from huggingface_hub) (21.3)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from huggingface_hub) (4.11.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface_hub) (4.1.1)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from huggingface_hub) (6.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from huggingface_hub) (2.27.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface_hub) (3.6.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.9->huggingface_hub) (3.0.7)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->huggingface_hub) (3.7.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface_hub) (3.3)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface_hub) (1.26.8)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface_hub) (2.0.12)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface_hub) (2021.10.8)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"model = timm.create_model('swin_base_patch4_window7_224', pretrained=True)\nmodel.head = nn.Sequential(\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, class_len)) # Modify head according to this task\n\nmodel = model.to(device)\n\ncriterion = LabelSmoothingCrossEntropy() # this is better than nn.CrossEntropyLoss\ncriterion = criterion.to(device)\n\noptimizer = torch.optim.AdamW(model.head.parameters(), lr=lr) # Setting for transfer learning","metadata":{"execution":{"iopub.status.busy":"2023-08-25T07:12:50.554114Z","iopub.execute_input":"2023-08-25T07:12:50.554757Z","iopub.status.idle":"2023-08-25T07:12:53.522204Z","shell.execute_reply.started":"2023-08-25T07:12:50.554709Z","shell.execute_reply":"2023-08-25T07:12:53.521019Z"},"trusted":true},"execution_count":28,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_17/2075558178.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'swin_base_patch4_window7_224'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m model.head = nn.Sequential(\n\u001b[1;32m      3\u001b[0m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/timm/models/_factory.py\u001b[0m in \u001b[0;36mcreate_model\u001b[0;34m(model_name, pretrained, pretrained_cfg, pretrained_cfg_overlay, checkpoint_path, scriptable, exportable, no_jit, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mpretrained_cfg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpretrained_cfg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0mpretrained_cfg_overlay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpretrained_cfg_overlay\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         )\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/timm/models/swin_transformer.py\u001b[0m in \u001b[0;36mswin_base_patch4_window7_224\u001b[0;34m(pretrained, **kwargs)\u001b[0m\n\u001b[1;32m    735\u001b[0m     \u001b[0mmodel_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m18\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m     return _create_swin_transformer(\n\u001b[0;32m--> 737\u001b[0;31m         'swin_base_patch4_window7_224', pretrained=pretrained, **dict(model_args, **kwargs))\n\u001b[0m\u001b[1;32m    738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/timm/models/swin_transformer.py\u001b[0m in \u001b[0;36m_create_swin_transformer\u001b[0;34m(variant, pretrained, **kwargs)\u001b[0m\n\u001b[1;32m    620\u001b[0m         \u001b[0mpretrained_filter_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_filter_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m         \u001b[0mfeature_cfg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflatten_sequential\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 622\u001b[0;31m         **kwargs)\n\u001b[0m\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/timm/models/_builder.py\u001b[0m in \u001b[0;36mbuild_model_with_cfg\u001b[0;34m(model_cls, variant, pretrained, pretrained_cfg, pretrained_cfg_overlay, model_cfg, feature_cfg, pretrained_strict, pretrained_filter_fn, kwargs_filter, **kwargs)\u001b[0m\n\u001b[1;32m    397\u001b[0m             \u001b[0min_chans\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'in_chans'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m             \u001b[0mfilter_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpretrained_filter_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m             \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpretrained_strict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m         )\n\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/timm/models/_builder.py\u001b[0m in \u001b[0;36mload_pretrained\u001b[0;34m(model, pretrained_cfg, num_classes, in_chans, filter_fn, strict)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid pretrained config, cannot load weights. Use `pretrained=False` for random init.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m     \u001b[0mload_from\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_loc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_resolve_pretrained_source\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_cfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mload_from\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'state_dict'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Loading pretrained weights from state dict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/timm/models/_builder.py\u001b[0m in \u001b[0;36m_resolve_pretrained_source\u001b[0;34m(pretrained_cfg)\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0;31m# prioritized old cached weights if exists and env var enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0mold_cache_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_cached_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_url\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpretrained_url\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mold_cache_valid\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhf_hub_id\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhas_hf_hub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnecessary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m                 \u001b[0;31m# hf-hub available as alternate weight source in default_cfg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mload_from\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'hf-hub'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/timm/models/_hub.py\u001b[0m in \u001b[0;36mhas_hf_hub\u001b[0;34m(necessary)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;31m# if no HF Hub module installed, and it is necessary to continue, raise error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         raise RuntimeError(\n\u001b[0;32m--> 113\u001b[0;31m             'Hugging Face hub model specified but package not installed. Run `pip install huggingface_hub`.')\n\u001b[0m\u001b[1;32m    114\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_has_hf_hub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Hugging Face hub model specified but package not installed. Run `pip install huggingface_hub`."],"ename":"RuntimeError","evalue":"Hugging Face hub model specified but package not installed. Run `pip install huggingface_hub`.","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}